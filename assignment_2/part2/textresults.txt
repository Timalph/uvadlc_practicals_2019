0.1 Accuracy: [0.028645833333333332, 0.041666666666666664, 0.04010416666666667, 0.0328125, 0.025, 0.027604166666666666, 0.026041666666666668, 0.028125, 0.03177083333333333, 0.023958333333333335, 0.026041666666666668, 0.10677083333333333, 0.029166666666666667, 0.030729166666666665, 0.030208333333333334, 0.0375, 0.033854166666666664, 0.03125, 0.03333333333333333, 0.030208333333333334, 0.040625, 0.0265625, 0.028125, 0.035416666666666666, 0.028645833333333332, 0.021354166666666667, 0.0359375, 0.028645833333333332, 0.035416666666666666, 0.03177083333333333, 0.034895833333333334, 0.034375, 0.03333333333333333, 0.024479166666666666, 0.020833333333333332, 0.029166666666666667, 0.028645833333333332, 0.028645833333333332, 0.040625, 0.034895833333333334, 0.030729166666666665, 0.028645833333333332, 0.0265625, 0.0359375, 0.029166666666666667, 0.027083333333333334, 0.030729166666666665, 0.10520833333333333, 0.030208333333333334, 0.0296875, 0.0328125, 0.03333333333333333, 0.0328125, 0.03854166666666667, 0.019791666666666666, 0.030208333333333334, 0.03177083333333333, 0.03177083333333333, 0.03697916666666667, 0.0203125, 0.0203125, 0.028125, 0.0328125, 0.0296875, 0.03125, 0.024479166666666666, 0.034375, 0.0296875, 0.0328125, 0.027604166666666666, 0.0359375, 0.021875, 0.0359375, 0.030208333333333334, 0.030729166666666665, 0.036458333333333336, 0.033854166666666664, 0.021875, 0.023958333333333335, 0.027083333333333334, 0.10416666666666667, 0.04114583333333333, 0.030208333333333334, 0.0359375, 0.0375, 0.028645833333333332, 0.03177083333333333, 0.03333333333333333, 0.030208333333333334, 0.030729166666666665, 0.030208333333333334, 0.035416666666666666, 0.03177083333333333, 0.027604166666666666, 0.03125, 0.033854166666666664, 0.025520833333333333, 0.03125, 0.03854166666666667, 0.028645833333333332, 0.030729166666666665, 0.0296875, 0.030208333333333334, 0.030208333333333334, 0.034895833333333334, 0.03697916666666667] Loss: [tensor(3.1029, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1450, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1426, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1019, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1093, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1389, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0644, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0623, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1478, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1768, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1322, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1551, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0833, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0913, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1005, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0763, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1730, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1016, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1415, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1004, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0956, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1098, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1172, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0794, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1494, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1815, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1082, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0916, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1111, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0982, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1019, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1284, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1071, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1431, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1359, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0974, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1229, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0900, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1209, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0909, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1060, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0872, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0861, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1371, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0773, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1356, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1412, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1317, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1318, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0849, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0978, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1245, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1623, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1069, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1825, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0944, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1248, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0906, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0726, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1401, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1347, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1307, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1304, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1229, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1057, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1406, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1362, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1129, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1024, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1693, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1087, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1568, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0623, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0974, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1273, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0844, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0959, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1550, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1633, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1359, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1403, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0880, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1306, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1578, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1141, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1450, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0820, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1082, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1136, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0773, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1463, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1042, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0788, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1626, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1108, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1204, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1677, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0883, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1008, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1206, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0899, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1134, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1410, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1197, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1055, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0685, device='cuda:0', grad_fn=<NllLoss2DBackward>)]
0.01 Accuracy: [0.13177083333333334, 0.0020833333333333333, 0.0046875, 0.021875, 0.0026041666666666665, 0.010416666666666666, 0.12916666666666668, 0.022916666666666665, 0.1296875, 0.1234375, 0.1328125, 0.008854166666666666, 0.1203125, 0.0067708333333333336, 0.13229166666666667, 0.11875, 0.009375, 0.009895833333333333, 0.003125, 0.0, 0.0020833333333333333, 0.004166666666666667, 0.008854166666666666, 0.009375, 0.004166666666666667, 0.013020833333333334, 0.13229166666666667, 0.011458333333333333, 0.12708333333333333, 0.0109375, 0.13229166666666667, 0.0078125, 0.0067708333333333336, 0.1328125, 0.014583333333333334, 0.009375, 0.0078125, 0.00625, 0.0005208333333333333, 0.1296875, 0.0026041666666666665, 0.0046875, 0.0005208333333333333, 0.0015625, 0.0010416666666666667, 0.11875, 0.128125, 0.13072916666666667, 0.015104166666666667, 0.009895833333333333, 0.005208333333333333, 0.01875, 0.12916666666666668, 0.013020833333333334, 0.0140625, 0.0078125, 0.13177083333333334, 0.00625, 0.016145833333333335, 0.12916666666666668, 0.0, 0.008854166666666666, 0.008333333333333333, 0.0036458333333333334, 0.022395833333333334, 0.0020833333333333333, 0.018229166666666668, 0.008333333333333333, 0.008854166666666666, 0.1328125, 0.007291666666666667, 0.009375, 0.009375, 0.0026041666666666665, 0.0067708333333333336, 0.005729166666666666, 0.009895833333333333, 0.1328125, 0.010416666666666666, 0.005729166666666666, 0.0078125, 0.017708333333333333, 0.12916666666666668, 0.13020833333333334, 0.0203125, 0.008333333333333333, 0.009375, 0.013020833333333334, 0.0020833333333333333, 0.015104166666666667, 0.0046875, 0.010416666666666666, 0.013541666666666667, 0.0036458333333333334, 0.011979166666666667, 0.0125, 0.0005208333333333333, 0.0203125, 0.016666666666666666, 0.009895833333333333, 0.0020833333333333333, 0.1328125, 0.011979166666666667, 0.1296875, 0.12135416666666667, 0.00625] Loss: [tensor(2.5669, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4537, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4348, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4323, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4171, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4092, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4486, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3909, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4436, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4844, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4435, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4446, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4513, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4277, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4185, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4545, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4362, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3954, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4305, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4475, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4141, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4519, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4130, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4268, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4639, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4330, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4100, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4012, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4209, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4216, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4185, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4221, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3980, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4301, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4624, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4256, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4068, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4176, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4228, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4151, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4104, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4468, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4243, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4388, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3888, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4758, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4494, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4051, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4280, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4224, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4286, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4091, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4149, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4416, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4402, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4073, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4392, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4053, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4076, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4439, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4043, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4218, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4104, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3797, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4184, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4341, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4013, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4104, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4397, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4645, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4111, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4363, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3863, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4106, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4430, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4124, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4139, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4518, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4482, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4335, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4137, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3820, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4656, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4202, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3903, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4274, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4035, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4195, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4048, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4081, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4322, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3802, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4017, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4259, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4176, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4191, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4384, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3860, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4011, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4287, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4313, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4313, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4370, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4449, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4301, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4101, device='cuda:0', grad_fn=<NllLoss2DBackward>)]
0.001 Accuracy: [0.029166666666666667, 0.10677083333333333, 0.1296875, 0.013020833333333334, 0.121875, 0.13229166666666667, 0.1296875, 0.011458333333333333, 0.003125, 0.12395833333333334, 0.13229166666666667, 0.0005208333333333333, 0.128125, 0.128125, 0.1234375, 0.125, 0.0046875, 0.008333333333333333, 0.00625, 0.0036458333333333334, 0.0, 0.007291666666666667, 0.009895833333333333, 0.1328125, 0.0036458333333333334, 0.010416666666666666, 0.004166666666666667, 0.0067708333333333336, 0.13020833333333334, 0.011458333333333333, 0.0015625, 0.009895833333333333, 0.0036458333333333334, 0.0010416666666666667, 0.008854166666666666, 0.0140625, 0.007291666666666667, 0.014583333333333334, 0.0020833333333333333, 0.12916666666666668, 0.12864583333333332, 0.00625, 0.0020833333333333333, 0.0020833333333333333, 0.0067708333333333336, 0.12135416666666667, 0.12760416666666666, 0.13072916666666667, 0.015625, 0.014583333333333334, 0.12708333333333333, 0.01875, 0.12760416666666666, 0.009895833333333333, 0.008854166666666666, 0.0, 0.0, 0.0036458333333333334, 0.010416666666666666, 0.12447916666666667, 0.0015625, 0.0109375, 0.008333333333333333, 0.0020833333333333333, 0.0234375, 0.003125, 0.01875, 0.005208333333333333, 0.008333333333333333, 0.13125, 0.0005208333333333333, 0.00625, 0.005208333333333333, 0.004166666666666667, 0.0046875, 0.004166666666666667, 0.0067708333333333336, 0.12291666666666666, 0.007291666666666667, 0.011979166666666667, 0.008333333333333333, 0.023958333333333335, 0.005208333333333333, 0.1296875, 0.022395833333333334, 0.008333333333333333, 0.016666666666666666, 0.0109375, 0.004166666666666667, 0.016666666666666666, 0.011458333333333333, 0.011458333333333333, 0.016666666666666666, 0.007291666666666667, 0.0125, 0.015104166666666667, 0.13020833333333334, 0.019270833333333334, 0.016145833333333335, 0.0067708333333333336, 0.004166666666666667, 0.13229166666666667, 0.010416666666666666, 0.1328125, 0.12083333333333333, 0.0] Loss: [tensor(3.0940, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.8833, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6625, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5810, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5372, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4983, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5213, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4529, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5140, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5361, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4931, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4994, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4656, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4632, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4467, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4747, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4580, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4102, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4654, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4737, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4319, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4727, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4168, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4501, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4827, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4569, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4213, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4135, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4268, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4341, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4232, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4320, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4057, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4431, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4818, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4282, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4232, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4217, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4391, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4143, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4157, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4510, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4279, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4444, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3952, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4862, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4558, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4123, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4296, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4283, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4308, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4128, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4219, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4487, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4489, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4137, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4528, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4145, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4141, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4451, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4029, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4330, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4212, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3842, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4223, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4329, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4027, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4166, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4411, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4692, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4141, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4353, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3898, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4155, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4429, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4151, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4132, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4618, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4514, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4399, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4146, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3851, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4585, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4241, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3977, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4339, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4043, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4235, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4095, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4064, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4260, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3817, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4012, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4228, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4212, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4191, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4393, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3842, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.3957, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4286, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4300, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4352, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4401, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4460, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4307, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4104, device='cuda:0', grad_fn=<NllLoss2DBackward>)]
0.0001 Accuracy: [0.027604166666666666, 0.03854166666666667, 0.0359375, 0.035416666666666666, 0.024479166666666666, 0.027083333333333334, 0.028125, 0.0265625, 0.0296875, 0.022916666666666665, 0.026041666666666668, 0.025520833333333333, 0.028125, 0.0296875, 0.028125, 0.0765625, 0.0640625, 0.0734375, 0.09791666666666667, 0.10885416666666667, 0.1125, 0.1234375, 0.11458333333333333, 0.12135416666666667, 0.1109375, 0.10833333333333334, 0.09895833333333333, 0.10885416666666667, 0.09375, 0.11927083333333334, 0.1125, 0.11979166666666667, 0.11875, 0.11927083333333334, 0.1109375, 0.12239583333333333, 0.12083333333333333, 0.13020833333333334, 0.121875, 0.11510416666666666, 0.1140625, 0.13072916666666667, 0.12604166666666666, 0.1203125, 0.115625, 0.10364583333333334, 0.1203125, 0.121875, 0.1265625, 0.13020833333333334, 0.125, 0.0026041666666666665, 0.11354166666666667, 0.13229166666666667, 0.1234375, 0.12760416666666666, 0.13020833333333334, 0.11458333333333333, 0.1328125, 0.11875, 0.10833333333333334, 0.0015625, 0.13072916666666667, 0.13072916666666667, 0.005208333333333333, 0.12604166666666666, 0.0026041666666666665, 0.12395833333333334, 0.12552083333333333, 0.12083333333333333, 0.12135416666666667, 0.12864583333333332, 0.13177083333333334, 0.0026041666666666665, 0.13020833333333334, 0.12760416666666666, 0.0036458333333333334, 0.11510416666666666, 0.1296875, 0.0026041666666666665, 0.12604166666666666, 0.009895833333333333, 0.12708333333333333, 0.1234375, 0.01875, 0.13020833333333334, 0.007291666666666667, 0.0109375, 0.0, 0.0125, 0.0078125, 0.007291666666666667, 0.0067708333333333336, 0.0, 0.013541666666666667, 0.008854166666666666, 0.13177083333333334, 0.01875, 0.015104166666666667, 0.0036458333333333334, 0.0026041666666666665, 0.11666666666666667, 0.005208333333333333, 0.13125, 0.12395833333333334, 0.0036458333333333334] Loss: [tensor(3.1666, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1701, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1386, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1126, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1103, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1358, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0523, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0592, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1331, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1612, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1110, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1204, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0426, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0415, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0025, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.9455, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.9787, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.8783, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.8917, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.8518, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.8106, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.8015, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7673, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7336, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7824, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7991, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7116, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6951, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7097, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7174, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7061, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6911, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6735, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7142, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.7253, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6651, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6553, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6320, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6607, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6513, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6547, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6442, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6203, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6445, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6013, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6905, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6513, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6142, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6461, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6098, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6244, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6188, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6221, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6275, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6519, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5749, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6044, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5823, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5532, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6018, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5922, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5855, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5751, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5469, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5622, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6030, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5760, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5591, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5773, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.6119, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5617, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5783, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5074, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5452, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5868, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5370, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5331, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5902, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5717, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5625, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5400, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4954, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5682, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5528, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4970, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5297, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4966, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5196, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5131, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4955, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5320, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4811, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4898, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5148, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5035, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5316, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5263, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4660, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4783, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5226, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5133, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5227, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5325, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5314, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.5134, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(2.4886, device='cuda:0', grad_fn=<NllLoss2DBackward>)]
0.00001 Accuracy: [0.0625, 0.028645833333333332, 0.03333333333333333, 0.028645833333333332, 0.021354166666666667, 0.0234375, 0.023958333333333335, 0.025, 0.028645833333333332, 0.0203125, 0.023958333333333335, 0.024479166666666666, 0.024479166666666666, 0.027604166666666666, 0.023958333333333335, 0.034895833333333334, 0.028125, 0.028645833333333332, 0.030729166666666665, 0.0265625, 0.03802083333333333, 0.025520833333333333, 0.025, 0.0296875, 0.026041666666666668, 0.01875, 0.034375, 0.026041666666666668, 0.03125, 0.028125, 0.030208333333333334, 0.03333333333333333, 0.03125, 0.022916666666666665, 0.020833333333333332, 0.026041666666666668, 0.025, 0.026041666666666668, 0.03854166666666667, 0.030729166666666665, 0.028125, 0.0265625, 0.0234375, 0.03333333333333333, 0.026041666666666668, 0.0234375, 0.026041666666666668, 0.021875, 0.024479166666666666, 0.027604166666666666, 0.030208333333333334, 0.0296875, 0.03177083333333333, 0.0328125, 0.019270833333333334, 0.025520833333333333, 0.027604166666666666, 0.030729166666666665, 0.0328125, 0.017708333333333333, 0.018229166666666668, 0.027083333333333334, 0.028645833333333332, 0.028645833333333332, 0.027083333333333334, 0.021875, 0.028645833333333332, 0.027604166666666666, 0.029166666666666667, 0.025520833333333333, 0.0328125, 0.020833333333333332, 0.034375, 0.027604166666666666, 0.028645833333333332, 0.033854166666666664, 0.03177083333333333, 0.01875, 0.022395833333333334, 0.0265625, 0.022395833333333334, 0.03958333333333333, 0.028125, 0.035416666666666666, 0.03854166666666667, 0.0296875, 0.030208333333333334, 0.03125, 0.0265625, 0.030208333333333334, 0.0296875, 0.03333333333333333, 0.03125, 0.0265625, 0.030208333333333334, 0.03125, 0.023958333333333335, 0.030208333333333334, 0.034375, 0.0265625, 0.029166666666666667, 0.028125, 0.025520833333333333, 0.028645833333333332, 0.03229166666666667, 0.034895833333333334] Loss: [tensor(4.2966, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(4.2405, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.5300, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.2929, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.2432, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.2461, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1502, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1433, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.2135, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.2272, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1822, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1948, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1345, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1528, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1449, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1296, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1879, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1351, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1640, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1583, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1366, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1541, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1503, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1195, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1813, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.2244, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1363, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1314, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1358, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1418, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1281, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1394, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1306, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1457, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1609, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1387, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1432, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1219, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1271, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1186, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1264, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1088, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0971, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1396, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1027, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1601, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1359, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1330, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1513, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1065, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1217, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1424, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1660, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1307, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1870, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1069, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1361, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1089, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0986, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1433, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1464, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1418, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1317, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1138, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1247, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1418, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1424, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1278, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1155, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1770, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1325, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1792, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0801, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1204, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1322, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0984, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1106, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1614, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1679, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1397, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1370, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1011, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1400, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1560, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1227, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1308, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0853, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1098, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1328, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0840, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1588, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1193, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0833, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1660, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1214, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1288, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1753, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1049, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1162, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1395, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1106, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1189, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1705, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1267, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.1093, device='cuda:0', grad_fn=<NllLoss2DBackward>), tensor(3.0800, device='cuda:0', grad_fn=<NllLoss2DBackward>)]
Test accuracy: <built-in method append of list object at 0x0000028B16249948> Learning rate: 0.0001 Accuracy: [0.08697916666666666, 0.1015625, 0.09479166666666666, 0.09791666666666667, 0.0953125, 0.0859375, 0.090625, 0.08541666666666667, 0.08958333333333333, 0.0984375] Loss: [3, 3, 3, 3, 2, 2, 2, 2, 2, 2]
Test accuracy: <built-in method append of list object at 0x0000028B08E4B048> Learning rate: 0.0001 Accuracy: [0.08697916666666666, 0.1015625, 0.09479166666666666, 0.09791666666666667, 0.0953125, 0.0859375, 0.090625, 0.08541666666666667, 0.08958333333333333, 0.0984375] Loss: [3, 3, 3, 3, 2, 2, 2, 2, 2, 2]
Test accuracy: <built-in method append of list object at 0x0000028B0BAC9148> Learning rate: 0.0001 Accuracy: [0.09010416666666667, 0.1015625, 0.09479166666666666, 0.09791666666666667, 0.0953125, 0.0859375, 0.090625, 0.08541666666666667, 0.08958333333333333, 0.0984375] Loss: [3, 3, 3, 3, 3, 2, 2, 2, 2, 2]
Test accuracy: <built-in method append of list object at 0x0000028B533F9788> Learning rate: 0.0001 Accuracy: [0.09010416666666667, 0.1015625, 0.09479166666666666, 0.09791666666666667, 0.0953125, 0.0859375, 0.090625, 0.08541666666666667, 0.08958333333333333, 0.0984375] Loss: [3, 3, 3, 3, 3, 2, 2, 2, 2, 2]
Test accuracy: <built-in method append of list object at 0x0000028B08E4B708> Learning rate: 0.0001 Accuracy: [0.09010416666666667, 0.1015625, 0.09479166666666666, 0.09791666666666667, 0.0953125, 0.0859375, 0.090625, 0.08541666666666667, 0.08958333333333333, 0.0984375] Loss: [3, 3, 3, 3, 3, 2, 2, 2, 2, 2]
Test accuracy: <built-in method append of list object at 0x000001F82603F788> Learning rate: 0.0001 Accuracy: [0.08958333333333333, 0.09739583333333333, 0.09375, 0.09791666666666667, 0.0953125, 0.0859375, 0.090625, 0.08541666666666667, 0.08958333333333333, 0.0984375] Loss: [3, 3, 3, 3, 3, 2, 2, 2, 3, 2]
Test accuracy: <built-in method append of list object at 0x000001F84EBD2108> Learning rate: 0.005 Accuracy: [0.11041666666666666, 0.07291666666666667, 0.13229166666666667, 0.05260416666666667, 0.09114583333333333, 0.10052083333333334, 0.12291666666666666, 0.125, 0.041666666666666664, 0.058854166666666666] Loss: [2, 1, 1, 1, 1, 1, 1, 1, 0, 0]
Test accuracy: <built-in method append of list object at 0x000001F82F43ACC8> Learning rate: 0.005 Accuracy: [0.11041666666666666, 0.07291666666666667, 0.13229166666666667, 0.05260416666666667, 0.09114583333333333, 0.10052083333333334, 0.12291666666666666, 0.125, 0.041666666666666664, 0.058854166666666666] Loss: [2, 1, 1, 1, 1, 1, 1, 1, 0, 0]
Test accuracy: <built-in method append of list object at 0x0000017F7DEC3888> Learning rate: 0.005 Accuracy: [0.240625, 0.36354166666666665, 0.4453125, 0.5046875, 0.5088541666666667, 0.5421875, 0.5244791666666667, 0.5666666666666667, 0.565625, 0.5619791666666667, 0.5557291666666667, 0.5817708333333333, 0.58125, 0.5895833333333333, 0.5864583333333333, 0.5890625, 0.5994791666666667, 0.5984375, 0.5953125, 0.5880208333333333, 0.6005208333333333, 0.5828125, 0.6072916666666667, 0.6078125, 0.6072916666666667, 0.6104166666666667, 0.6291666666666667, 0.6192708333333333, 0.6057291666666667, 0.6083333333333333, 0.6213541666666667, 0.6395833333333333, 0.6182291666666667, 0.6223958333333334, 0.6223958333333334, 0.6083333333333333, 0.6270833333333333, 0.6046875, 0.6140625, 0.6333333333333333, 0.6208333333333333, 0.6322916666666667, 0.6229166666666667, 0.6098958333333333, 0.6380208333333334, 0.6119791666666666, 0.6026041666666667, 0.6260416666666667, 0.6302083333333334, 0.6302083333333334, 0.6416666666666667, 0.6223958333333334, 0.6416666666666667, 0.6322916666666667, 0.6359375, 0.63125, 0.6348958333333333, 0.6364583333333333, 0.6104166666666667, 0.621875, 0.6213541666666667, 0.6494791666666667, 0.65, 0.6479166666666667, 0.6328125, 0.6479166666666667, 0.6395833333333333, 0.6375, 0.6354166666666666, 0.6255208333333333, 0.6364583333333333, 0.6239583333333333, 0.6276041666666666, 0.6354166666666666, 0.6255208333333333, 0.6401041666666667, 0.6270833333333333, 0.6255208333333333, 0.6567708333333333, 0.6348958333333333, 0.6385416666666667, 0.6411458333333333, 0.6333333333333333, 0.6286458333333333, 0.6296875, 0.6302083333333334, 0.64375, 0.6395833333333333, 0.6369791666666667, 0.6385416666666667, 0.6479166666666667, 0.6484375, 0.6260416666666667, 0.6322916666666667, 0.6411458333333333, 0.646875, 0.6197916666666666, 0.628125, 0.6421875, 0.6427083333333333, 0.6328125, 0.6583333333333333, 0.6348958333333333, 0.6390625, 0.6453125, 0.6359375] Loss: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x000001FBF447C508> Learning rate: 0.005 Accuracy: [0.33645833333333336, 0.4395833333333333, 0.49166666666666664, 0.5640625, 0.6171875, 0.6192708333333333, 0.6494791666666667, 0.6473958333333333, 0.7026041666666667, 0.7135416666666666] Loss: [2, 1, 1, 1, 1, 1, 1, 1, 1, 0]
Test accuracy: <built-in method append of list object at 0x000002AEA0839988> Learning rate: 0.005 Accuracy: [0.1625, 0.24947916666666667, 0.37083333333333335, 0.41770833333333335, 0.45677083333333335, 0.48802083333333335, 0.4869791666666667, 0.5348958333333333, 0.5296875, 0.5286458333333334, 0.534375, 0.5755208333333334, 0.5385416666666667, 0.5635416666666667, 0.5765625, 0.5796875, 0.5807291666666666, 0.5869791666666667, 0.575, 0.5734375, 0.5885416666666666, 0.5723958333333333, 0.5932291666666667, 0.5875, 0.5895833333333333, 0.6026041666666667, 0.6098958333333333, 0.6145833333333334, 0.59375, 0.6, 0.6171875, 0.6208333333333333, 0.6151041666666667, 0.61875, 0.6177083333333333, 0.6036458333333333, 0.6182291666666667, 0.5916666666666667, 0.6145833333333334, 0.60625, 0.6177083333333333, 0.6255208333333333, 0.6145833333333334, 0.59375, 0.6286458333333333, 0.5942708333333333, 0.5973958333333333, 0.6109375, 0.625, 0.6041666666666666, 0.6234375, 0.615625, 0.6255208333333333, 0.6328125, 0.634375, 0.6270833333333333, 0.6203125, 0.6333333333333333, 0.6104166666666667, 0.628125, 0.6192708333333333, 0.6479166666666667, 0.6494791666666667, 0.6546875, 0.63125, 0.6515625, 0.6265625, 0.6291666666666667, 0.6276041666666666, 0.6151041666666667, 0.6239583333333333, 0.6171875, 0.6369791666666667, 0.6302083333333334, 0.6135416666666667, 0.6348958333333333, 0.6208333333333333, 0.6197916666666666, 0.6411458333333333, 0.6182291666666667, 0.6244791666666667, 0.6348958333333333, 0.6390625, 0.6255208333333333, 0.6364583333333333, 0.61875, 0.6463541666666667, 0.6338541666666667, 0.63125, 0.6317708333333333, 0.6489583333333333, 0.6484375, 0.6192708333333333, 0.6197916666666666, 0.6458333333333334, 0.6338541666666667, 0.6244791666666667, 0.6317708333333333, 0.6286458333333333, 0.6473958333333333, 0.6291666666666667, 0.6541666666666667, 0.6223958333333334, 0.6307291666666667, 0.640625, 0.6328125] Loss: [3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x000002AE87D0B508> Learning rate: 0.005 Accuracy: [0.23958333333333334, 0.3328125, 0.4234375, 0.49270833333333336, 0.5182291666666666, 0.54375, 0.5703125, 0.6046875, 0.61875, 0.6182291666666667] Loss: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x000002AE9E4405C8> Learning rate: 0.005 Accuracy: [0.1625, 0.24947916666666667, 0.37083333333333335, 0.41770833333333335, 0.45677083333333335, 0.48802083333333335, 0.4869791666666667, 0.5348958333333333, 0.5296875, 0.5286458333333334, 0.534375, 0.5755208333333334, 0.5385416666666667, 0.5635416666666667, 0.5765625, 0.5796875, 0.5807291666666666, 0.5869791666666667, 0.575, 0.5734375, 0.5885416666666666, 0.5723958333333333, 0.5932291666666667, 0.5875, 0.5895833333333333, 0.6026041666666667, 0.6098958333333333, 0.6145833333333334, 0.59375, 0.6, 0.6171875, 0.6208333333333333, 0.6151041666666667, 0.61875, 0.6177083333333333, 0.6036458333333333, 0.6182291666666667, 0.5916666666666667, 0.6145833333333334, 0.60625, 0.6177083333333333, 0.6255208333333333, 0.6145833333333334, 0.59375, 0.6286458333333333, 0.5942708333333333, 0.5973958333333333, 0.6109375, 0.625, 0.6041666666666666, 0.6234375, 0.615625, 0.6255208333333333, 0.6328125, 0.634375, 0.6270833333333333, 0.6203125, 0.6333333333333333, 0.6104166666666667, 0.628125, 0.6192708333333333, 0.6479166666666667, 0.6494791666666667, 0.6546875, 0.63125, 0.6515625, 0.6265625, 0.6291666666666667, 0.6276041666666666, 0.6151041666666667, 0.6239583333333333, 0.6171875, 0.6369791666666667, 0.6302083333333334, 0.6135416666666667, 0.6348958333333333, 0.6208333333333333, 0.6197916666666666, 0.6411458333333333, 0.6182291666666667, 0.6244791666666667, 0.6348958333333333, 0.6390625, 0.6255208333333333, 0.6364583333333333, 0.61875, 0.6463541666666667, 0.6338541666666667, 0.63125, 0.6317708333333333, 0.6489583333333333, 0.6484375, 0.6192708333333333, 0.6197916666666666, 0.6458333333333334, 0.6338541666666667, 0.6244791666666667, 0.6317708333333333, 0.6286458333333333, 0.6473958333333333, 0.6291666666666667, 0.6541666666666667, 0.6223958333333334, 0.6307291666666667, 0.640625, 0.6328125] Loss: [3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x000002AE87D26848> Learning rate: 0.005 Accuracy: [0.23958333333333334, 0.3328125, 0.4234375, 0.49270833333333336, 0.5182291666666666, 0.54375, 0.5703125, 0.6046875, 0.61875, 0.6182291666666667] Loss: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x000001A98C80A988> Learning rate: 0.005 Accuracy: [0.3619791666666667, 0.4484375, 0.5052083333333334, 0.5520833333333334, 0.5390625, 0.5635416666666667, 0.5494791666666666, 0.5953125, 0.5864583333333333, 0.5817708333333333, 0.5635416666666667, 0.6067708333333334, 0.5859375, 0.5859375, 0.6005208333333333, 0.5963541666666666, 0.60625, 0.609375, 0.6052083333333333, 0.5947916666666667, 0.609375, 0.5869791666666667, 0.6067708333333334, 0.6151041666666667, 0.6072916666666667, 0.6119791666666666, 0.6354166666666666, 0.6286458333333333, 0.6151041666666667, 0.6223958333333334, 0.6296875, 0.6427083333333333, 0.6239583333333333, 0.6291666666666667, 0.6239583333333333, 0.6177083333333333, 0.625, 0.609375, 0.6213541666666667, 0.6255208333333333, 0.6171875, 0.6359375, 0.6182291666666667, 0.61875, 0.6458333333333334, 0.6083333333333333, 0.625, 0.6223958333333334, 0.640625, 0.628125, 0.6479166666666667, 0.6265625, 0.6286458333333333, 0.6322916666666667, 0.6411458333333333, 0.6421875, 0.640625, 0.6458333333333334, 0.6166666666666667, 0.628125, 0.615625, 0.6572916666666667, 0.6604166666666667, 0.65, 0.6296875, 0.6526041666666667, 0.6380208333333334, 0.63125, 0.6296875, 0.615625, 0.634375, 0.6234375, 0.6453125, 0.6421875, 0.6302083333333334, 0.64375, 0.621875, 0.6317708333333333, 0.6494791666666667, 0.6375, 0.6385416666666667, 0.6427083333333333, 0.6395833333333333, 0.6291666666666667, 0.6515625, 0.6348958333333333, 0.65, 0.64375, 0.6375, 0.6458333333333334, 0.6557291666666667, 0.65625, 0.6244791666666667, 0.6317708333333333, 0.65, 0.6453125, 0.6348958333333333, 0.6317708333333333, 0.6411458333333333, 0.653125, 0.6375, 0.6536458333333334, 0.6447916666666667, 0.6354166666666666, 0.6541666666666667, 0.6317708333333333] Loss: [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x0000019882509B08> Learning rate: 0.005 Accuracy: [0.31354166666666666, 0.4510416666666667, 0.49635416666666665, 0.55, 0.55, 0.5796875, 0.5526041666666667, 0.5979166666666667, 0.5729166666666666, 0.5802083333333333, 0.575, 0.5932291666666667, 0.5796875, 0.5833333333333334, 0.5989583333333334, 0.5994791666666667, 0.6052083333333333, 0.6135416666666667, 0.5973958333333333, 0.5848958333333333, 0.6151041666666667, 0.5885416666666666, 0.6125, 0.6197916666666666, 0.6083333333333333, 0.6119791666666666, 0.6317708333333333, 0.6291666666666667, 0.6067708333333334, 0.6104166666666667, 0.6317708333333333, 0.6369791666666667, 0.6229166666666667, 0.621875, 0.6270833333333333, 0.6072916666666667, 0.6197916666666666, 0.6078125, 0.6255208333333333, 0.6328125, 0.6223958333333334, 0.6390625, 0.6307291666666667, 0.6114583333333333, 0.6458333333333334, 0.615625, 0.6114583333333333, 0.6328125, 0.6375, 0.6197916666666666, 0.6401041666666667, 0.6140625, 0.6296875, 0.6385416666666667, 0.6442708333333333, 0.640625, 0.6380208333333334, 0.6427083333333333, 0.621875, 0.6338541666666667, 0.6348958333333333, 0.653125, 0.6546875, 0.6536458333333334, 0.6244791666666667, 0.653125, 0.64375, 0.6421875, 0.6427083333333333, 0.6197916666666666, 0.6364583333333333, 0.621875, 0.6359375, 0.6427083333333333, 0.6239583333333333, 0.6411458333333333, 0.6296875, 0.634375, 0.6567708333333333, 0.6395833333333333, 0.6375, 0.65625, 0.6411458333333333, 0.6328125, 0.6401041666666667, 0.63125, 0.6609375, 0.640625, 0.634375, 0.6328125, 0.6541666666666667, 0.6619791666666667, 0.6328125, 0.6239583333333333, 0.6354166666666666, 0.6416666666666667, 0.6296875, 0.6260416666666667, 0.6390625, 0.6572916666666667, 0.64375, 0.6510416666666666, 0.6390625, 0.6385416666666667, 0.646875, 0.64375] Loss: [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x0000019882587488> Learning rate: 0.005 Accuracy: [0.31354166666666666, 0.4510416666666667, 0.49635416666666665, 0.55, 0.55, 0.5796875, 0.5526041666666667, 0.5979166666666667, 0.5729166666666666, 0.5802083333333333, 0.575, 0.5932291666666667, 0.5796875, 0.5833333333333334, 0.5989583333333334, 0.5994791666666667, 0.6052083333333333, 0.6135416666666667, 0.5973958333333333, 0.5848958333333333, 0.6151041666666667, 0.5885416666666666, 0.6125, 0.6197916666666666, 0.6083333333333333, 0.6119791666666666, 0.6317708333333333, 0.6291666666666667, 0.6067708333333334, 0.6104166666666667, 0.6317708333333333, 0.6369791666666667, 0.6229166666666667, 0.621875, 0.6270833333333333, 0.6072916666666667, 0.6197916666666666, 0.6078125, 0.6255208333333333, 0.6328125, 0.6223958333333334, 0.6390625, 0.6307291666666667, 0.6114583333333333, 0.6458333333333334, 0.615625, 0.6114583333333333, 0.6328125, 0.6375, 0.6197916666666666, 0.6401041666666667, 0.6140625, 0.6296875, 0.6385416666666667, 0.6442708333333333, 0.640625, 0.6380208333333334, 0.6427083333333333, 0.621875, 0.6338541666666667, 0.6348958333333333, 0.653125, 0.6546875, 0.6536458333333334, 0.6244791666666667, 0.653125, 0.64375, 0.6421875, 0.6427083333333333, 0.6197916666666666, 0.6364583333333333, 0.621875, 0.6359375, 0.6427083333333333, 0.6239583333333333, 0.6411458333333333, 0.6296875, 0.634375, 0.6567708333333333, 0.6395833333333333, 0.6375, 0.65625, 0.6411458333333333, 0.6328125, 0.6401041666666667, 0.63125, 0.6609375, 0.640625, 0.634375, 0.6328125, 0.6541666666666667, 0.6619791666666667, 0.6328125, 0.6239583333333333, 0.6354166666666666, 0.6416666666666667, 0.6296875, 0.6260416666666667, 0.6390625, 0.6572916666666667, 0.64375, 0.6510416666666666, 0.6390625, 0.6385416666666667, 0.646875, 0.64375] Loss: [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x00000292022C2148> Learning rate: 0.005 Accuracy: [0.33229166666666665, 0.43385416666666665, 0.48333333333333334, 0.5411458333333333, 0.5520833333333334, 0.5796875, 0.5463541666666667, 0.5921875, 0.5796875, 0.5723958333333333, 0.5604166666666667, 0.6057291666666667, 0.596875, 0.6052083333333333, 0.5921875, 0.5869791666666667, 0.6005208333333333, 0.6109375, 0.603125, 0.5953125, 0.6109375, 0.590625, 0.6072916666666667, 0.6177083333333333, 0.6197916666666666, 0.6104166666666667, 0.6286458333333333, 0.6328125, 0.6114583333333333, 0.6265625, 0.6328125, 0.6359375, 0.6229166666666667, 0.6265625, 0.621875, 0.6161458333333333, 0.6203125, 0.6192708333333333, 0.6130208333333333, 0.6260416666666667, 0.6244791666666667, 0.6442708333333333, 0.6260416666666667, 0.6104166666666667, 0.6421875, 0.6020833333333333, 0.6135416666666667, 0.6317708333333333, 0.634375, 0.6359375, 0.6411458333333333, 0.6354166666666666, 0.64375, 0.6338541666666667, 0.6380208333333334, 0.6364583333333333, 0.6317708333333333, 0.6421875, 0.6057291666666667, 0.628125, 0.6286458333333333, 0.6541666666666667, 0.6619791666666667, 0.6473958333333333, 0.6291666666666667, 0.6505208333333333, 0.6359375, 0.6333333333333333, 0.6348958333333333, 0.6213541666666667, 0.6453125, 0.625, 0.6322916666666667, 0.6364583333333333, 0.6229166666666667, 0.646875, 0.6333333333333333, 0.6276041666666666, 0.6630208333333333, 0.6260416666666667, 0.6473958333333333, 0.6307291666666667, 0.6416666666666667, 0.6354166666666666, 0.6416666666666667, 0.6338541666666667, 0.6572916666666667, 0.6447916666666667, 0.646875, 0.6385416666666667, 0.65625, 0.6526041666666667, 0.6151041666666667, 0.6322916666666667, 0.6385416666666667, 0.634375, 0.6322916666666667, 0.6348958333333333, 0.6390625, 0.6494791666666667, 0.6447916666666667, 0.6635416666666667, 0.6390625, 0.6432291666666666, 0.6479166666666667, 0.634375] Loss: [2.361018419265747, 1.9040700197219849, 1.7340092658996582, 1.5756704807281494, 1.4989503622055054, 1.4147881269454956, 1.5111674070358276, 1.3714057207107544, 1.38902747631073, 1.4268807172775269, 1.4153366088867188, 1.3306775093078613, 1.33748197555542, 1.325791835784912, 1.3148497343063354, 1.3264598846435547, 1.2836929559707642, 1.272162675857544, 1.3257981538772583, 1.303505301475525, 1.2423615455627441, 1.358440637588501, 1.2661406993865967, 1.2393772602081299, 1.2541289329528809, 1.2217519283294678, 1.2063649892807007, 1.185608148574829, 1.2235913276672363, 1.2174307107925415, 1.2005454301834106, 1.164487361907959, 1.2118834257125854, 1.2146766185760498, 1.2192797660827637, 1.232941746711731, 1.1922622919082642, 1.2574442625045776, 1.2031744718551636, 1.212257742881775, 1.1916649341583252, 1.1788629293441772, 1.2118302583694458, 1.233739972114563, 1.1510909795761108, 1.242173671722412, 1.2369771003723145, 1.1952182054519653, 1.1571869850158691, 1.2079391479492188, 1.1400229930877686, 1.205086350440979, 1.1630595922470093, 1.1547244787216187, 1.1818697452545166, 1.1285563707351685, 1.1863688230514526, 1.1426711082458496, 1.2115082740783691, 1.1615054607391357, 1.191007375717163, 1.114063024520874, 1.130370855331421, 1.1163897514343262, 1.151430368423462, 1.1197316646575928, 1.159665584564209, 1.1547303199768066, 1.167280912399292, 1.1919175386428833, 1.147264003753662, 1.1995928287506104, 1.1410683393478394, 1.141284704208374, 1.1964048147201538, 1.1287459135055542, 1.15846848487854, 1.1885545253753662, 1.0880481004714966, 1.1728500127792358, 1.1528240442276, 1.1551614999771118, 1.1496450901031494, 1.1714149713516235, 1.1158896684646606, 1.1607571840286255, 1.1139681339263916, 1.1167534589767456, 1.13336980342865, 1.153944492340088, 1.1158992052078247, 1.1145867109298706, 1.173591136932373, 1.1746928691864014, 1.1753489971160889, 1.14031183719635, 1.1564457416534424, 1.1621507406234741, 1.1487525701522827, 1.118469476699829, 1.1313997507095337, 1.077853798866272, 1.156746506690979, 1.1594740152359009, 1.1149051189422607, 1.155158281326294]
Test accuracy: <built-in method append of list object at 0x00000292022C2148> Learning rate: 0.005 Accuracy: [0.33229166666666665, 0.43385416666666665, 0.48333333333333334, 0.5411458333333333, 0.5520833333333334, 0.5796875, 0.5463541666666667, 0.5921875, 0.5796875, 0.5723958333333333, 0.5604166666666667, 0.6057291666666667, 0.596875, 0.6052083333333333, 0.5921875, 0.5869791666666667, 0.6005208333333333, 0.6109375, 0.603125, 0.5953125, 0.6109375, 0.590625, 0.6072916666666667, 0.6177083333333333, 0.6197916666666666, 0.6104166666666667, 0.6286458333333333, 0.6328125, 0.6114583333333333, 0.6265625, 0.6328125, 0.6359375, 0.6229166666666667, 0.6265625, 0.621875, 0.6161458333333333, 0.6203125, 0.6192708333333333, 0.6130208333333333, 0.6260416666666667, 0.6244791666666667, 0.6442708333333333, 0.6260416666666667, 0.6104166666666667, 0.6421875, 0.6020833333333333, 0.6135416666666667, 0.6317708333333333, 0.634375, 0.6359375, 0.6411458333333333, 0.6354166666666666, 0.64375, 0.6338541666666667, 0.6380208333333334, 0.6364583333333333, 0.6317708333333333, 0.6421875, 0.6057291666666667, 0.628125, 0.6286458333333333, 0.6541666666666667, 0.6619791666666667, 0.6473958333333333, 0.6291666666666667, 0.6505208333333333, 0.6359375, 0.6333333333333333, 0.6348958333333333, 0.6213541666666667, 0.6453125, 0.625, 0.6322916666666667, 0.6364583333333333, 0.6229166666666667, 0.646875, 0.6333333333333333, 0.6276041666666666, 0.6630208333333333, 0.6260416666666667, 0.6473958333333333, 0.6307291666666667, 0.6416666666666667, 0.6354166666666666, 0.6416666666666667, 0.6338541666666667, 0.6572916666666667, 0.6447916666666667, 0.646875, 0.6385416666666667, 0.65625, 0.6526041666666667, 0.6151041666666667, 0.6322916666666667, 0.6385416666666667, 0.634375, 0.6322916666666667, 0.6348958333333333, 0.6390625, 0.6494791666666667, 0.6447916666666667, 0.6635416666666667, 0.6390625, 0.6432291666666666, 0.6479166666666667, 0.634375, 0.6572916666666667, 0.6614583333333334, 0.6692708333333334, 0.6796875, 0.6604166666666667, 0.6526041666666667, 0.6536458333333334, 0.6708333333333333, 0.6682291666666667, 0.6536458333333334, 0.653125, 0.66875, 0.65, 0.65625, 0.6541666666666667, 0.6494791666666667, 0.6609375, 0.66875, 0.6677083333333333, 0.6484375, 0.66875, 0.6479166666666667, 0.6682291666666667, 0.6619791666666667, 0.665625, 0.6588541666666666, 0.6713541666666667, 0.6703125, 0.6692708333333334, 0.6682291666666667, 0.6817708333333333, 0.6833333333333333, 0.6713541666666667, 0.6697916666666667, 0.671875, 0.6635416666666667, 0.6786458333333333, 0.65625, 0.6692708333333334, 0.659375, 0.6640625, 0.6770833333333334, 0.66875, 0.6546875, 0.6822916666666666, 0.6546875, 0.6463541666666667, 0.6755208333333333, 0.6854166666666667, 0.6666666666666666, 0.6765625, 0.6609375, 0.6770833333333334, 0.6744791666666666, 0.675, 0.6734375, 0.6723958333333333, 0.6802083333333333, 0.6479166666666667, 0.6755208333333333, 0.6651041666666667, 0.6880208333333333, 0.6859375, 0.68125, 0.6578125, 0.6739583333333333, 0.6755208333333333, 0.6625, 0.6713541666666667, 0.6505208333333333, 0.6765625, 0.6625, 0.6828125, 0.6760416666666667, 0.6598958333333333, 0.6755208333333333, 0.6729166666666667, 0.6604166666666667, 0.6854166666666667, 0.665625, 0.6692708333333334, 0.6765625, 0.6770833333333334, 0.6588541666666666, 0.6822916666666666, 0.6682291666666667, 0.6921875, 0.6760416666666667, 0.6671875, 0.6786458333333333, 0.6927083333333334, 0.6807291666666667, 0.6609375, 0.6604166666666667, 0.6838541666666667, 0.6760416666666667, 0.6578125, 0.66875, 0.6713541666666667, 0.6807291666666667, 0.6703125, 0.6880208333333333, 0.6661458333333333, 0.6661458333333333, 0.6729166666666667, 0.6619791666666667] Loss: [2.361018419265747, 1.9040700197219849, 1.7340092658996582, 1.5756704807281494, 1.4989503622055054, 1.4147881269454956, 1.5111674070358276, 1.3714057207107544, 1.38902747631073, 1.4268807172775269, 1.4153366088867188, 1.3306775093078613, 1.33748197555542, 1.325791835784912, 1.3148497343063354, 1.3264598846435547, 1.2836929559707642, 1.272162675857544, 1.3257981538772583, 1.303505301475525, 1.2423615455627441, 1.358440637588501, 1.2661406993865967, 1.2393772602081299, 1.2541289329528809, 1.2217519283294678, 1.2063649892807007, 1.185608148574829, 1.2235913276672363, 1.2174307107925415, 1.2005454301834106, 1.164487361907959, 1.2118834257125854, 1.2146766185760498, 1.2192797660827637, 1.232941746711731, 1.1922622919082642, 1.2574442625045776, 1.2031744718551636, 1.212257742881775, 1.1916649341583252, 1.1788629293441772, 1.2118302583694458, 1.233739972114563, 1.1510909795761108, 1.242173671722412, 1.2369771003723145, 1.1952182054519653, 1.1571869850158691, 1.2079391479492188, 1.1400229930877686, 1.205086350440979, 1.1630595922470093, 1.1547244787216187, 1.1818697452545166, 1.1285563707351685, 1.1863688230514526, 1.1426711082458496, 1.2115082740783691, 1.1615054607391357, 1.191007375717163, 1.114063024520874, 1.130370855331421, 1.1163897514343262, 1.151430368423462, 1.1197316646575928, 1.159665584564209, 1.1547303199768066, 1.167280912399292, 1.1919175386428833, 1.147264003753662, 1.1995928287506104, 1.1410683393478394, 1.141284704208374, 1.1964048147201538, 1.1287459135055542, 1.15846848487854, 1.1885545253753662, 1.0880481004714966, 1.1728500127792358, 1.1528240442276, 1.1551614999771118, 1.1496450901031494, 1.1714149713516235, 1.1158896684646606, 1.1607571840286255, 1.1139681339263916, 1.1167534589767456, 1.13336980342865, 1.153944492340088, 1.1158992052078247, 1.1145867109298706, 1.173591136932373, 1.1746928691864014, 1.1753489971160889, 1.14031183719635, 1.1564457416534424, 1.1621507406234741, 1.1487525701522827, 1.118469476699829, 1.1313997507095337, 1.077853798866272, 1.156746506690979, 1.1594740152359009, 1.1149051189422607, 1.155158281326294, 1.1138492822647095, 1.0812267065048218, 1.0745161771774292, 1.045070767402649, 1.0802704095840454, 1.0704652070999146, 1.1093024015426636, 1.0734783411026, 1.0804808139801025, 1.1163588762283325, 1.1371201276779175, 1.0833327770233154, 1.0839712619781494, 1.0932648181915283, 1.082884430885315, 1.1098779439926147, 1.0637519359588623, 1.0683411359786987, 1.0970500707626343, 1.1065620183944702, 1.039648175239563, 1.1620467901229858, 1.067644476890564, 1.0823339223861694, 1.071029782295227, 1.0563222169876099, 1.0412474870681763, 1.046580195426941, 1.0600272417068481, 1.063859462738037, 1.0506705045700073, 1.0108305215835571, 1.0423007011413574, 1.0611178874969482, 1.066825032234192, 1.0746694803237915, 1.0311291217803955, 1.0980887413024902, 1.0478254556655884, 1.0915799140930176, 1.0665932893753052, 1.019211769104004, 1.0806260108947754, 1.0898113250732422, 1.0304720401763916, 1.1024894714355469, 1.1054267883300781, 1.0536525249481201, 1.0346760749816895, 1.0601580142974854, 1.0140300989151, 1.0709960460662842, 1.0377886295318604, 1.0374208688735962, 1.055462121963501, 1.0025984048843384, 1.071535587310791, 1.0233670473098755, 1.0876296758651733, 1.0403462648391724, 1.0611506700515747, 0.9994627833366394, 1.0286747217178345, 1.0091445446014404, 1.043250322341919, 1.0203009843826294, 1.0417282581329346, 1.0437837839126587, 1.0663833618164062, 1.0634667873382568, 1.0524877309799194, 1.0731148719787598, 1.028896450996399, 1.0319441556930542, 1.075929045677185, 1.0348339080810547, 1.0420154333114624, 1.068346381187439, 0.9871692061424255, 1.063147783279419, 1.0561412572860718, 1.0540052652359009, 1.0310713052749634, 1.074344515800476, 1.005974292755127, 1.0579447746276855, 1.0102022886276245, 1.0366677045822144, 1.0373756885528564, 1.0294383764266968, 1.0125501155853271, 1.0231404304504395, 1.0565425157546997, 1.0732098817825317, 1.0615748167037964, 1.0402487516403198, 1.061295986175537, 1.0580158233642578, 1.0713766813278198, 1.0222489833831787, 1.0640718936920166, 1.0037357807159424, 1.0752170085906982, 1.0700660943984985, 1.0322673320770264, 1.0649964809417725]
Test accuracy: <built-in method append of list object at 0x00000292022C2148> Learning rate: 0.005 Accuracy: [0.33229166666666665, 0.43385416666666665, 0.48333333333333334, 0.5411458333333333, 0.5520833333333334, 0.5796875, 0.5463541666666667, 0.5921875, 0.5796875, 0.5723958333333333, 0.5604166666666667, 0.6057291666666667, 0.596875, 0.6052083333333333, 0.5921875, 0.5869791666666667, 0.6005208333333333, 0.6109375, 0.603125, 0.5953125, 0.6109375, 0.590625, 0.6072916666666667, 0.6177083333333333, 0.6197916666666666, 0.6104166666666667, 0.6286458333333333, 0.6328125, 0.6114583333333333, 0.6265625, 0.6328125, 0.6359375, 0.6229166666666667, 0.6265625, 0.621875, 0.6161458333333333, 0.6203125, 0.6192708333333333, 0.6130208333333333, 0.6260416666666667, 0.6244791666666667, 0.6442708333333333, 0.6260416666666667, 0.6104166666666667, 0.6421875, 0.6020833333333333, 0.6135416666666667, 0.6317708333333333, 0.634375, 0.6359375, 0.6411458333333333, 0.6354166666666666, 0.64375, 0.6338541666666667, 0.6380208333333334, 0.6364583333333333, 0.6317708333333333, 0.6421875, 0.6057291666666667, 0.628125, 0.6286458333333333, 0.6541666666666667, 0.6619791666666667, 0.6473958333333333, 0.6291666666666667, 0.6505208333333333, 0.6359375, 0.6333333333333333, 0.6348958333333333, 0.6213541666666667, 0.6453125, 0.625, 0.6322916666666667, 0.6364583333333333, 0.6229166666666667, 0.646875, 0.6333333333333333, 0.6276041666666666, 0.6630208333333333, 0.6260416666666667, 0.6473958333333333, 0.6307291666666667, 0.6416666666666667, 0.6354166666666666, 0.6416666666666667, 0.6338541666666667, 0.6572916666666667, 0.6447916666666667, 0.646875, 0.6385416666666667, 0.65625, 0.6526041666666667, 0.6151041666666667, 0.6322916666666667, 0.6385416666666667, 0.634375, 0.6322916666666667, 0.6348958333333333, 0.6390625, 0.6494791666666667, 0.6447916666666667, 0.6635416666666667, 0.6390625, 0.6432291666666666, 0.6479166666666667, 0.634375, 0.6572916666666667, 0.6614583333333334, 0.6692708333333334, 0.6796875, 0.6604166666666667, 0.6526041666666667, 0.6536458333333334, 0.6708333333333333, 0.6682291666666667, 0.6536458333333334, 0.653125, 0.66875, 0.65, 0.65625, 0.6541666666666667, 0.6494791666666667, 0.6609375, 0.66875, 0.6677083333333333, 0.6484375, 0.66875, 0.6479166666666667, 0.6682291666666667, 0.6619791666666667, 0.665625, 0.6588541666666666, 0.6713541666666667, 0.6703125, 0.6692708333333334, 0.6682291666666667, 0.6817708333333333, 0.6833333333333333, 0.6713541666666667, 0.6697916666666667, 0.671875, 0.6635416666666667, 0.6786458333333333, 0.65625, 0.6692708333333334, 0.659375, 0.6640625, 0.6770833333333334, 0.66875, 0.6546875, 0.6822916666666666, 0.6546875, 0.6463541666666667, 0.6755208333333333, 0.6854166666666667, 0.6666666666666666, 0.6765625, 0.6609375, 0.6770833333333334, 0.6744791666666666, 0.675, 0.6734375, 0.6723958333333333, 0.6802083333333333, 0.6479166666666667, 0.6755208333333333, 0.6651041666666667, 0.6880208333333333, 0.6859375, 0.68125, 0.6578125, 0.6739583333333333, 0.6755208333333333, 0.6625, 0.6713541666666667, 0.6505208333333333, 0.6765625, 0.6625, 0.6828125, 0.6760416666666667, 0.6598958333333333, 0.6755208333333333, 0.6729166666666667, 0.6604166666666667, 0.6854166666666667, 0.665625, 0.6692708333333334, 0.6765625, 0.6770833333333334, 0.6588541666666666, 0.6822916666666666, 0.6682291666666667, 0.6921875, 0.6760416666666667, 0.6671875, 0.6786458333333333, 0.6927083333333334, 0.6807291666666667, 0.6609375, 0.6604166666666667, 0.6838541666666667, 0.6760416666666667, 0.6578125, 0.66875, 0.6713541666666667, 0.6807291666666667, 0.6703125, 0.6880208333333333, 0.6661458333333333, 0.6661458333333333, 0.6729166666666667, 0.6619791666666667, 0.6734375, 0.6734375, 0.6911458333333333, 0.6947916666666667, 0.68125, 0.6723958333333333, 0.6625, 0.671875, 0.6776041666666667, 0.6640625, 0.6739583333333333, 0.6734375, 0.6635416666666667, 0.6739583333333333, 0.6692708333333334, 0.6583333333333333, 0.6765625, 0.6828125, 0.6755208333333333, 0.6583333333333333, 0.6833333333333333, 0.6567708333333333, 0.68125, 0.6739583333333333, 0.6755208333333333, 0.6713541666666667, 0.6677083333333333, 0.684375, 0.6776041666666667, 0.6776041666666667, 0.690625, 0.6901041666666666, 0.678125, 0.6729166666666667, 0.6760416666666667, 0.6723958333333333, 0.6890625, 0.6604166666666667, 0.6744791666666666, 0.665625, 0.6692708333333334, 0.6869791666666667, 0.6760416666666667, 0.66875, 0.6848958333333334, 0.6625, 0.6552083333333333, 0.6828125, 0.6875, 0.6744791666666666, 0.684375, 0.6645833333333333, 0.6755208333333333, 0.68125, 0.6854166666666667, 0.6880208333333333, 0.6744791666666666, 0.6947916666666667, 0.6567708333333333, 0.6776041666666667, 0.6755208333333333, 0.6932291666666667, 0.690625, 0.6864583333333333, 0.6723958333333333, 0.6911458333333333, 0.6833333333333333, 0.6739583333333333, 0.6770833333333334, 0.6635416666666667, 0.6822916666666666, 0.6692708333333334, 0.6838541666666667, 0.6822916666666666, 0.6666666666666666, 0.6802083333333333, 0.6734375, 0.6635416666666667, 0.6921875, 0.665625, 0.6760416666666667, 0.6802083333333333, 0.6833333333333333, 0.6666666666666666, 0.6859375, 0.671875, 0.69375, 0.6765625, 0.6734375, 0.6875, 0.6911458333333333, 0.6796875, 0.6635416666666667, 0.6645833333333333, 0.6864583333333333, 0.6703125, 0.6645833333333333, 0.6640625, 0.6692708333333334, 0.6848958333333334, 0.6697916666666667, 0.6911458333333333, 0.6645833333333333, 0.6734375, 0.6682291666666667, 0.66875] Loss: [2.361018419265747, 1.9040700197219849, 1.7340092658996582, 1.5756704807281494, 1.4989503622055054, 1.4147881269454956, 1.5111674070358276, 1.3714057207107544, 1.38902747631073, 1.4268807172775269, 1.4153366088867188, 1.3306775093078613, 1.33748197555542, 1.325791835784912, 1.3148497343063354, 1.3264598846435547, 1.2836929559707642, 1.272162675857544, 1.3257981538772583, 1.303505301475525, 1.2423615455627441, 1.358440637588501, 1.2661406993865967, 1.2393772602081299, 1.2541289329528809, 1.2217519283294678, 1.2063649892807007, 1.185608148574829, 1.2235913276672363, 1.2174307107925415, 1.2005454301834106, 1.164487361907959, 1.2118834257125854, 1.2146766185760498, 1.2192797660827637, 1.232941746711731, 1.1922622919082642, 1.2574442625045776, 1.2031744718551636, 1.212257742881775, 1.1916649341583252, 1.1788629293441772, 1.2118302583694458, 1.233739972114563, 1.1510909795761108, 1.242173671722412, 1.2369771003723145, 1.1952182054519653, 1.1571869850158691, 1.2079391479492188, 1.1400229930877686, 1.205086350440979, 1.1630595922470093, 1.1547244787216187, 1.1818697452545166, 1.1285563707351685, 1.1863688230514526, 1.1426711082458496, 1.2115082740783691, 1.1615054607391357, 1.191007375717163, 1.114063024520874, 1.130370855331421, 1.1163897514343262, 1.151430368423462, 1.1197316646575928, 1.159665584564209, 1.1547303199768066, 1.167280912399292, 1.1919175386428833, 1.147264003753662, 1.1995928287506104, 1.1410683393478394, 1.141284704208374, 1.1964048147201538, 1.1287459135055542, 1.15846848487854, 1.1885545253753662, 1.0880481004714966, 1.1728500127792358, 1.1528240442276, 1.1551614999771118, 1.1496450901031494, 1.1714149713516235, 1.1158896684646606, 1.1607571840286255, 1.1139681339263916, 1.1167534589767456, 1.13336980342865, 1.153944492340088, 1.1158992052078247, 1.1145867109298706, 1.173591136932373, 1.1746928691864014, 1.1753489971160889, 1.14031183719635, 1.1564457416534424, 1.1621507406234741, 1.1487525701522827, 1.118469476699829, 1.1313997507095337, 1.077853798866272, 1.156746506690979, 1.1594740152359009, 1.1149051189422607, 1.155158281326294, 1.1138492822647095, 1.0812267065048218, 1.0745161771774292, 1.045070767402649, 1.0802704095840454, 1.0704652070999146, 1.1093024015426636, 1.0734783411026, 1.0804808139801025, 1.1163588762283325, 1.1371201276779175, 1.0833327770233154, 1.0839712619781494, 1.0932648181915283, 1.082884430885315, 1.1098779439926147, 1.0637519359588623, 1.0683411359786987, 1.0970500707626343, 1.1065620183944702, 1.039648175239563, 1.1620467901229858, 1.067644476890564, 1.0823339223861694, 1.071029782295227, 1.0563222169876099, 1.0412474870681763, 1.046580195426941, 1.0600272417068481, 1.063859462738037, 1.0506705045700073, 1.0108305215835571, 1.0423007011413574, 1.0611178874969482, 1.066825032234192, 1.0746694803237915, 1.0311291217803955, 1.0980887413024902, 1.0478254556655884, 1.0915799140930176, 1.0665932893753052, 1.019211769104004, 1.0806260108947754, 1.0898113250732422, 1.0304720401763916, 1.1024894714355469, 1.1054267883300781, 1.0536525249481201, 1.0346760749816895, 1.0601580142974854, 1.0140300989151, 1.0709960460662842, 1.0377886295318604, 1.0374208688735962, 1.055462121963501, 1.0025984048843384, 1.071535587310791, 1.0233670473098755, 1.0876296758651733, 1.0403462648391724, 1.0611506700515747, 0.9994627833366394, 1.0286747217178345, 1.0091445446014404, 1.043250322341919, 1.0203009843826294, 1.0417282581329346, 1.0437837839126587, 1.0663833618164062, 1.0634667873382568, 1.0524877309799194, 1.0731148719787598, 1.028896450996399, 1.0319441556930542, 1.075929045677185, 1.0348339080810547, 1.0420154333114624, 1.068346381187439, 0.9871692061424255, 1.063147783279419, 1.0561412572860718, 1.0540052652359009, 1.0310713052749634, 1.074344515800476, 1.005974292755127, 1.0579447746276855, 1.0102022886276245, 1.0366677045822144, 1.0373756885528564, 1.0294383764266968, 1.0125501155853271, 1.0231404304504395, 1.0565425157546997, 1.0732098817825317, 1.0615748167037964, 1.0402487516403198, 1.061295986175537, 1.0580158233642578, 1.0713766813278198, 1.0222489833831787, 1.0640718936920166, 1.0037357807159424, 1.0752170085906982, 1.0700660943984985, 1.0322673320770264, 1.0649964809417725, 1.05803382396698, 1.0309289693832397, 1.0192071199417114, 1.0064641237258911, 1.0434248447418213, 1.0263181924819946, 1.0626312494277954, 1.0507689714431763, 1.035132884979248, 1.0732674598693848, 1.0788137912750244, 1.045143485069275, 1.0416516065597534, 1.0455491542816162, 1.048606038093567, 1.0752530097961426, 1.0314738750457764, 1.0282832384109497, 1.0602601766586304, 1.0773053169250488, 1.0063843727111816, 1.1196229457855225, 1.0359998941421509, 1.060950756072998, 1.0372532606124878, 1.0212215185165405, 1.0183593034744263, 1.0162980556488037, 1.0334196090698242, 1.0404326915740967, 1.0222238302230835, 0.9896482229232788, 1.0120558738708496, 1.0364429950714111, 1.044298768043518, 1.0472670793533325, 1.0076690912246704, 1.0782334804534912, 1.0257596969604492, 1.070976734161377, 1.0478874444961548, 0.9953149557113647, 1.057069182395935, 1.0616676807403564, 1.0118235349655151, 1.0819467306137085, 1.0778769254684448, 1.0310605764389038, 1.011393427848816, 1.0408742427825928, 0.9936937093734741, 1.0522743463516235, 1.023709774017334, 1.0164741277694702, 1.0340598821640015, 0.9781097173690796, 1.0510612726211548, 1.0003540515899658, 1.0696296691894531, 1.0195063352584839, 1.0401191711425781, 0.9839308857917786, 1.0144490003585815, 0.9984683394432068, 1.0286126136779785, 1.0014408826828003, 1.023504376411438, 1.0290560722351074, 1.0485022068023682, 1.038760781288147, 1.0372148752212524, 1.0594830513000488, 1.0180104970932007, 1.0194443464279175, 1.0594608783721924, 1.02333402633667, 1.0278126001358032, 1.0547398328781128, 0.9755316972732544, 1.0478019714355469, 1.038250207901001, 1.0394748449325562, 1.0236570835113525, 1.0599991083145142, 0.9909893274307251, 1.0464513301849365, 0.9984468817710876, 1.0280365943908691, 1.027060866355896, 1.0148447751998901, 1.0024569034576416, 1.009458065032959, 1.0450314283370972, 1.0583136081695557, 1.0477161407470703, 1.0321451425552368, 1.0552598237991333, 1.0478293895721436, 1.064476728439331, 1.0157173871994019, 1.0542235374450684, 0.9976349472999573, 1.0692291259765625, 1.0634920597076416, 1.0273805856704712, 1.054847002029419]
Test accuracy: <built-in method append of list object at 0x0000018E11A82B08> Learning rate: 0.005 Accuracy: [0.33541666666666664, 0.42135416666666664, 0.47291666666666665, 0.525, 0.5395833333333333] Loss: [2, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x0000018E11A8D608> Learning rate: 0.005 Accuracy: [0.5369791666666667, 0.5864583333333333, 0.5723958333333333, 0.5927083333333333, 0.590625] Loss: [1, 1, 1, 1, 1]
Test accuracy: <built-in method append of list object at 0x0000018E11B027C8> Learning rate: 0.005 Accuracy: [0.33541666666666664] Loss: [2.338351249694824]
Test accuracy: <built-in method append of list object at 0x0000018E11B027C8> Learning rate: 0.005 Accuracy: [0.33541666666666664, 0.42135416666666664] Loss: [2.338351249694824, 2.0145459175109863]
Test accuracy: <built-in method append of list object at 0x0000018E11B027C8> Learning rate: 0.005 Accuracy: [0.33541666666666664, 0.42135416666666664, 0.4817708333333333] Loss: [2.338351249694824, 2.0145459175109863, 1.7816976308822632]
Test accuracy: <built-in method append of list object at 0x0000018E11B07208> Learning rate: 0.005 Accuracy: [0.33541666666666664, 0.42135416666666664, 0.47291666666666665, 0.525, 0.5395833333333333, 0.5588541666666667, 0.5484375, 0.5895833333333333, 0.5755208333333334, 0.5786458333333333, 0.5744791666666667, 0.5984375, 0.5833333333333334, 0.5911458333333334, 0.5994791666666667, 0.5994791666666667, 0.609375, 0.615625, 0.5984375, 0.5875, 0.6145833333333334, 0.5958333333333333, 0.6166666666666667, 0.6213541666666667, 0.6041666666666666, 0.6208333333333333, 0.6291666666666667, 0.6197916666666666, 0.6041666666666666, 0.6145833333333334, 0.6380208333333334, 0.6338541666666667, 0.625, 0.6270833333333333, 0.6317708333333333, 0.6036458333333333, 0.625, 0.6104166666666667, 0.6166666666666667, 0.63125, 0.6265625, 0.6442708333333333, 0.6317708333333333, 0.6125, 0.6432291666666666, 0.6166666666666667, 0.6161458333333333, 0.6302083333333334, 0.6484375, 0.6239583333333333, 0.6479166666666667, 0.6255208333333333, 0.6270833333333333, 0.6390625, 0.63125, 0.6359375, 0.6270833333333333, 0.6442708333333333, 0.6213541666666667, 0.625, 0.6260416666666667, 0.6578125, 0.6583333333333333, 0.64375, 0.6380208333333334, 0.6421875, 0.6375, 0.6364583333333333, 0.6359375, 0.625, 0.6322916666666667, 0.621875, 0.6333333333333333, 0.6442708333333333, 0.6265625, 0.6489583333333333, 0.634375, 0.6265625, 0.6567708333333333, 0.6260416666666667, 0.6380208333333334, 0.6359375, 0.6369791666666667, 0.6364583333333333, 0.6489583333333333, 0.6265625, 0.659375, 0.6390625, 0.6364583333333333, 0.6463541666666667, 0.6510416666666666, 0.6489583333333333, 0.6296875, 0.6234375, 0.640625, 0.6453125, 0.6260416666666667, 0.6317708333333333, 0.6354166666666666, 0.6473958333333333, 0.6432291666666666, 0.6526041666666667, 0.63125, 0.6427083333333333, 0.65, 0.6395833333333333] Loss: [2.338351249694824, 1.9620929956436157, 1.7606773376464844, 1.6134753227233887, 1.532369613647461, 1.4719957113265991, 1.5098655223846436, 1.3824858665466309, 1.4183859825134277, 1.4343187808990479, 1.4111443758010864, 1.3304475545883179, 1.3694133758544922, 1.326088547706604, 1.3099141120910645, 1.3150845766067505, 1.2731913328170776, 1.264093041419983, 1.3280748128890991, 1.3159348964691162, 1.2356926202774048, 1.3673540353775024, 1.2602665424346924, 1.2376328706741333, 1.2796051502227783, 1.2238253355026245, 1.2091535329818726, 1.2020947933197021, 1.2382514476776123, 1.2239093780517578, 1.2028509378433228, 1.187124252319336, 1.2289257049560547, 1.1903051137924194, 1.2044090032577515, 1.2411627769470215, 1.1892215013504028, 1.2658562660217285, 1.1975646018981934, 1.2079997062683105, 1.1994794607162476, 1.1797175407409668, 1.2203519344329834, 1.2252451181411743, 1.165885329246521, 1.2501323223114014, 1.244278073310852, 1.1873661279678345, 1.1580657958984375, 1.2247276306152344, 1.1432141065597534, 1.2095718383789062, 1.158509373664856, 1.1655535697937012, 1.1659176349639893, 1.1490447521209717, 1.1967219114303589, 1.1390101909637451, 1.2221544981002808, 1.1698622703552246, 1.1883546113967896, 1.1086103916168213, 1.1209269762039185, 1.1354777812957764, 1.1392765045166016, 1.1299264430999756, 1.154364824295044, 1.1481118202209473, 1.1652874946594238, 1.1858667135238647, 1.15487802028656, 1.1847248077392578, 1.1525014638900757, 1.1462546586990356, 1.208011507987976, 1.120740532875061, 1.1659481525421143, 1.1694896221160889, 1.0828713178634644, 1.1748626232147217, 1.1503666639328003, 1.1466387510299683, 1.1454447507858276, 1.1696397066116333, 1.1216487884521484, 1.1778172254562378, 1.12550687789917, 1.1234385967254639, 1.1306135654449463, 1.151374340057373, 1.129196047782898, 1.1178187131881714, 1.170576572418213, 1.1899133920669556, 1.1707265377044678, 1.1164737939834595, 1.1959823369979858, 1.1681346893310547, 1.1587492227554321, 1.13798189163208, 1.1310572624206543, 1.101757526397705, 1.1414200067520142, 1.1491819620132446, 1.1347318887710571, 1.144582986831665]
Test accuracy: <built-in method append of list object at 0x0000018E11B07208> Learning rate: 0.005 Accuracy: [0.33541666666666664, 0.42135416666666664, 0.47291666666666665, 0.525, 0.5395833333333333, 0.5588541666666667, 0.5484375, 0.5895833333333333, 0.5755208333333334, 0.5786458333333333, 0.5744791666666667, 0.5984375, 0.5833333333333334, 0.5911458333333334, 0.5994791666666667, 0.5994791666666667, 0.609375, 0.615625, 0.5984375, 0.5875, 0.6145833333333334, 0.5958333333333333, 0.6166666666666667, 0.6213541666666667, 0.6041666666666666, 0.6208333333333333, 0.6291666666666667, 0.6197916666666666, 0.6041666666666666, 0.6145833333333334, 0.6380208333333334, 0.6338541666666667, 0.625, 0.6270833333333333, 0.6317708333333333, 0.6036458333333333, 0.625, 0.6104166666666667, 0.6166666666666667, 0.63125, 0.6265625, 0.6442708333333333, 0.6317708333333333, 0.6125, 0.6432291666666666, 0.6166666666666667, 0.6161458333333333, 0.6302083333333334, 0.6484375, 0.6239583333333333, 0.6479166666666667, 0.6255208333333333, 0.6270833333333333, 0.6390625, 0.63125, 0.6359375, 0.6270833333333333, 0.6442708333333333, 0.6213541666666667, 0.625, 0.6260416666666667, 0.6578125, 0.6583333333333333, 0.64375, 0.6380208333333334, 0.6421875, 0.6375, 0.6364583333333333, 0.6359375, 0.625, 0.6322916666666667, 0.621875, 0.6333333333333333, 0.6442708333333333, 0.6265625, 0.6489583333333333, 0.634375, 0.6265625, 0.6567708333333333, 0.6260416666666667, 0.6380208333333334, 0.6359375, 0.6369791666666667, 0.6364583333333333, 0.6489583333333333, 0.6265625, 0.659375, 0.6390625, 0.6364583333333333, 0.6463541666666667, 0.6510416666666666, 0.6489583333333333, 0.6296875, 0.6234375, 0.640625, 0.6453125, 0.6260416666666667, 0.6317708333333333, 0.6354166666666666, 0.6473958333333333, 0.6432291666666666, 0.6526041666666667, 0.63125, 0.6427083333333333, 0.65, 0.6395833333333333, 0.6213541666666667, 0.6458333333333334, 0.6380208333333334, 0.6640625, 0.6442708333333333, 0.6359375, 0.6244791666666667, 0.6489583333333333, 0.6380208333333334, 0.640625, 0.6234375, 0.6536458333333334, 0.6369791666666667, 0.6317708333333333, 0.6427083333333333, 0.6302083333333334, 0.6427083333333333, 0.646875, 0.6338541666666667, 0.6296875, 0.6515625, 0.63125, 0.6453125, 0.6510416666666666, 0.6520833333333333, 0.6479166666666667, 0.6598958333333333, 0.6552083333333333, 0.6395833333333333, 0.6395833333333333, 0.6515625, 0.6598958333333333, 0.6479166666666667, 0.6447916666666667, 0.6536458333333334, 0.6338541666666667, 0.6567708333333333, 0.6229166666666667, 0.6572916666666667, 0.6494791666666667, 0.6338541666666667, 0.6666666666666666, 0.653125, 0.6375, 0.6510416666666666, 0.6375, 0.6260416666666667, 0.6536458333333334, 0.6479166666666667, 0.6328125, 0.6640625, 0.6515625, 0.6520833333333333, 0.6583333333333333, 0.6578125, 0.6520833333333333, 0.6494791666666667, 0.6640625, 0.6333333333333333, 0.6494791666666667, 0.6411458333333333, 0.6723958333333333, 0.671875, 0.659375, 0.6484375, 0.6703125, 0.6520833333333333, 0.6526041666666667, 0.6536458333333334, 0.6359375, 0.6421875, 0.6359375, 0.653125, 0.6494791666666667, 0.6322916666666667, 0.6635416666666667, 0.6494791666666667, 0.6416666666666667, 0.6677083333333333, 0.6453125, 0.653125, 0.6494791666666667, 0.6609375, 0.6526041666666667, 0.6598958333333333, 0.6416666666666667, 0.6692708333333334, 0.6427083333333333, 0.6515625, 0.6578125, 0.6630208333333333, 0.6666666666666666, 0.6463541666666667, 0.640625, 0.6458333333333334, 0.6557291666666667, 0.634375, 0.6494791666666667, 0.6411458333333333, 0.6536458333333334, 0.6505208333333333, 0.6609375, 0.6515625, 0.65625, 0.6619791666666667, 0.6416666666666667] Loss: [2.338351249694824, 1.9620929956436157, 1.7606773376464844, 1.6134753227233887, 1.532369613647461, 1.4719957113265991, 1.5098655223846436, 1.3824858665466309, 1.4183859825134277, 1.4343187808990479, 1.4111443758010864, 1.3304475545883179, 1.3694133758544922, 1.326088547706604, 1.3099141120910645, 1.3150845766067505, 1.2731913328170776, 1.264093041419983, 1.3280748128890991, 1.3159348964691162, 1.2356926202774048, 1.3673540353775024, 1.2602665424346924, 1.2376328706741333, 1.2796051502227783, 1.2238253355026245, 1.2091535329818726, 1.2020947933197021, 1.2382514476776123, 1.2239093780517578, 1.2028509378433228, 1.187124252319336, 1.2289257049560547, 1.1903051137924194, 1.2044090032577515, 1.2411627769470215, 1.1892215013504028, 1.2658562660217285, 1.1975646018981934, 1.2079997062683105, 1.1994794607162476, 1.1797175407409668, 1.2203519344329834, 1.2252451181411743, 1.165885329246521, 1.2501323223114014, 1.244278073310852, 1.1873661279678345, 1.1580657958984375, 1.2247276306152344, 1.1432141065597534, 1.2095718383789062, 1.158509373664856, 1.1655535697937012, 1.1659176349639893, 1.1490447521209717, 1.1967219114303589, 1.1390101909637451, 1.2221544981002808, 1.1698622703552246, 1.1883546113967896, 1.1086103916168213, 1.1209269762039185, 1.1354777812957764, 1.1392765045166016, 1.1299264430999756, 1.154364824295044, 1.1481118202209473, 1.1652874946594238, 1.1858667135238647, 1.15487802028656, 1.1847248077392578, 1.1525014638900757, 1.1462546586990356, 1.208011507987976, 1.120740532875061, 1.1659481525421143, 1.1694896221160889, 1.0828713178634644, 1.1748626232147217, 1.1503666639328003, 1.1466387510299683, 1.1454447507858276, 1.1696397066116333, 1.1216487884521484, 1.1778172254562378, 1.12550687789917, 1.1234385967254639, 1.1306135654449463, 1.151374340057373, 1.129196047782898, 1.1178187131881714, 1.170576572418213, 1.1899133920669556, 1.1707265377044678, 1.1164737939834595, 1.1959823369979858, 1.1681346893310547, 1.1587492227554321, 1.13798189163208, 1.1310572624206543, 1.101757526397705, 1.1414200067520142, 1.1491819620132446, 1.1347318887710571, 1.144582986831665, 1.2135539054870605, 1.1528359651565552, 1.1491725444793701, 1.0971646308898926, 1.1299971342086792, 1.1272705793380737, 1.1823867559432983, 1.107822299003601, 1.143808126449585, 1.183708667755127, 1.182350993156433, 1.1464463472366333, 1.1554292440414429, 1.1578797101974487, 1.142924189567566, 1.1583760976791382, 1.116803526878357, 1.1217275857925415, 1.1599228382110596, 1.1602386236190796, 1.0928070545196533, 1.2064967155456543, 1.1276291608810425, 1.0988706350326538, 1.134400725364685, 1.108317255973816, 1.0871161222457886, 1.0933092832565308, 1.1262867450714111, 1.1333701610565186, 1.1039879322052002, 1.073742389678955, 1.1127699613571167, 1.1287953853607178, 1.1257998943328857, 1.1457877159118652, 1.0797920227050781, 1.152708888053894, 1.0901762247085571, 1.1254538297653198, 1.1279784440994263, 1.096714735031128, 1.1294496059417725, 1.149990200996399, 1.085931658744812, 1.1651684045791626, 1.1656564474105835, 1.099032998085022, 1.0967556238174438, 1.1532440185546875, 1.058794617652893, 1.1413557529449463, 1.0955511331558228, 1.0858861207962036, 1.0878689289093018, 1.0953019857406616, 1.112755298614502, 1.0730915069580078, 1.1533280611038208, 1.0966721773147583, 1.1258833408355713, 1.0475966930389404, 1.076920747756958, 1.0718048810958862, 1.0825611352920532, 1.0711054801940918, 1.1107962131500244, 1.1145089864730835, 1.11556875705719, 1.1427000761032104, 1.1194103956222534, 1.128803014755249, 1.1054472923278809, 1.0895687341690063, 1.1486767530441284, 1.066338062286377, 1.090377926826477, 1.1212176084518433, 1.0430529117584229, 1.134710669517517, 1.1049774885177612, 1.1108638048171997, 1.0857901573181152, 1.131887435913086, 1.0732016563415527, 1.1171092987060547, 1.0897361040115356, 1.092767596244812, 1.0834623575210571, 1.1164450645446777, 1.0804239511489868, 1.0586599111557007, 1.1228883266448975, 1.1364542245864868, 1.1316262483596802, 1.0791130065917969, 1.148840308189392, 1.1143279075622559, 1.1345672607421875, 1.0880457162857056, 1.1035290956497192, 1.070821762084961, 1.1139042377471924, 1.1046146154403687, 1.0876104831695557, 1.1114853620529175]
Test accuracy: <built-in method append of list object at 0x0000018E11B07208> Learning rate: 0.005 Accuracy: [0.33541666666666664, 0.42135416666666664, 0.47291666666666665, 0.525, 0.5395833333333333, 0.5588541666666667, 0.5484375, 0.5895833333333333, 0.5755208333333334, 0.5786458333333333, 0.5744791666666667, 0.5984375, 0.5833333333333334, 0.5911458333333334, 0.5994791666666667, 0.5994791666666667, 0.609375, 0.615625, 0.5984375, 0.5875, 0.6145833333333334, 0.5958333333333333, 0.6166666666666667, 0.6213541666666667, 0.6041666666666666, 0.6208333333333333, 0.6291666666666667, 0.6197916666666666, 0.6041666666666666, 0.6145833333333334, 0.6380208333333334, 0.6338541666666667, 0.625, 0.6270833333333333, 0.6317708333333333, 0.6036458333333333, 0.625, 0.6104166666666667, 0.6166666666666667, 0.63125, 0.6265625, 0.6442708333333333, 0.6317708333333333, 0.6125, 0.6432291666666666, 0.6166666666666667, 0.6161458333333333, 0.6302083333333334, 0.6484375, 0.6239583333333333, 0.6479166666666667, 0.6255208333333333, 0.6270833333333333, 0.6390625, 0.63125, 0.6359375, 0.6270833333333333, 0.6442708333333333, 0.6213541666666667, 0.625, 0.6260416666666667, 0.6578125, 0.6583333333333333, 0.64375, 0.6380208333333334, 0.6421875, 0.6375, 0.6364583333333333, 0.6359375, 0.625, 0.6322916666666667, 0.621875, 0.6333333333333333, 0.6442708333333333, 0.6265625, 0.6489583333333333, 0.634375, 0.6265625, 0.6567708333333333, 0.6260416666666667, 0.6380208333333334, 0.6359375, 0.6369791666666667, 0.6364583333333333, 0.6489583333333333, 0.6265625, 0.659375, 0.6390625, 0.6364583333333333, 0.6463541666666667, 0.6510416666666666, 0.6489583333333333, 0.6296875, 0.6234375, 0.640625, 0.6453125, 0.6260416666666667, 0.6317708333333333, 0.6354166666666666, 0.6473958333333333, 0.6432291666666666, 0.6526041666666667, 0.63125, 0.6427083333333333, 0.65, 0.6395833333333333, 0.6213541666666667, 0.6458333333333334, 0.6380208333333334, 0.6640625, 0.6442708333333333, 0.6359375, 0.6244791666666667, 0.6489583333333333, 0.6380208333333334, 0.640625, 0.6234375, 0.6536458333333334, 0.6369791666666667, 0.6317708333333333, 0.6427083333333333, 0.6302083333333334, 0.6427083333333333, 0.646875, 0.6338541666666667, 0.6296875, 0.6515625, 0.63125, 0.6453125, 0.6510416666666666, 0.6520833333333333, 0.6479166666666667, 0.6598958333333333, 0.6552083333333333, 0.6395833333333333, 0.6395833333333333, 0.6515625, 0.6598958333333333, 0.6479166666666667, 0.6447916666666667, 0.6536458333333334, 0.6338541666666667, 0.6567708333333333, 0.6229166666666667, 0.6572916666666667, 0.6494791666666667, 0.6338541666666667, 0.6666666666666666, 0.653125, 0.6375, 0.6510416666666666, 0.6375, 0.6260416666666667, 0.6536458333333334, 0.6479166666666667, 0.6328125, 0.6640625, 0.6515625, 0.6520833333333333, 0.6583333333333333, 0.6578125, 0.6520833333333333, 0.6494791666666667, 0.6640625, 0.6333333333333333, 0.6494791666666667, 0.6411458333333333, 0.6723958333333333, 0.671875, 0.659375, 0.6484375, 0.6703125, 0.6520833333333333, 0.6526041666666667, 0.6536458333333334, 0.6359375, 0.6421875, 0.6359375, 0.653125, 0.6494791666666667, 0.6322916666666667, 0.6635416666666667, 0.6494791666666667, 0.6416666666666667, 0.6677083333333333, 0.6453125, 0.653125, 0.6494791666666667, 0.6609375, 0.6526041666666667, 0.6598958333333333, 0.6416666666666667, 0.6692708333333334, 0.6427083333333333, 0.6515625, 0.6578125, 0.6630208333333333, 0.6666666666666666, 0.6463541666666667, 0.640625, 0.6458333333333334, 0.6557291666666667, 0.634375, 0.6494791666666667, 0.6411458333333333, 0.6536458333333334, 0.6505208333333333, 0.6609375, 0.6515625, 0.65625, 0.6619791666666667, 0.6416666666666667, 0.6229166666666667, 0.65, 0.6510416666666666, 0.671875, 0.6536458333333334, 0.6463541666666667, 0.6328125, 0.6479166666666667, 0.6515625, 0.6421875, 0.6364583333333333, 0.6520833333333333, 0.634375, 0.6453125, 0.6505208333333333, 0.6447916666666667, 0.65625, 0.6619791666666667, 0.640625, 0.63125, 0.6635416666666667, 0.6458333333333334, 0.6505208333333333, 0.6489583333333333, 0.6609375, 0.6447916666666667, 0.6630208333333333, 0.6692708333333334, 0.6479166666666667, 0.653125, 0.66875, 0.6703125, 0.6557291666666667, 0.6526041666666667, 0.6635416666666667, 0.6515625, 0.6661458333333333, 0.64375, 0.65625, 0.6494791666666667, 0.64375, 0.6640625, 0.6541666666666667, 0.6411458333333333, 0.6666666666666666, 0.6369791666666667, 0.6286458333333333, 0.6604166666666667, 0.6609375, 0.6385416666666667, 0.6697916666666667, 0.6484375, 0.65625, 0.6651041666666667, 0.66875, 0.653125, 0.65625, 0.675, 0.6432291666666666, 0.65625, 0.6458333333333334, 0.66875, 0.684375, 0.6651041666666667, 0.6567708333333333, 0.6838541666666667, 0.6640625, 0.6567708333333333, 0.6588541666666666, 0.6479166666666667, 0.6411458333333333, 0.6453125, 0.6583333333333333, 0.6588541666666666, 0.6442708333333333, 0.6661458333333333, 0.64375, 0.6395833333333333, 0.6817708333333333, 0.6578125, 0.6625, 0.6598958333333333, 0.6671875, 0.6546875, 0.6661458333333333, 0.6505208333333333, 0.6604166666666667, 0.6588541666666666, 0.6572916666666667, 0.6578125, 0.6713541666666667, 0.6755208333333333, 0.6557291666666667, 0.6348958333333333, 0.659375, 0.6635416666666667, 0.6447916666666667, 0.6588541666666666, 0.65, 0.6640625, 0.659375, 0.66875, 0.6546875, 0.6583333333333333, 0.6619791666666667, 0.6520833333333333] Loss: [2.338351249694824, 1.9620929956436157, 1.7606773376464844, 1.6134753227233887, 1.532369613647461, 1.4719957113265991, 1.5098655223846436, 1.3824858665466309, 1.4183859825134277, 1.4343187808990479, 1.4111443758010864, 1.3304475545883179, 1.3694133758544922, 1.326088547706604, 1.3099141120910645, 1.3150845766067505, 1.2731913328170776, 1.264093041419983, 1.3280748128890991, 1.3159348964691162, 1.2356926202774048, 1.3673540353775024, 1.2602665424346924, 1.2376328706741333, 1.2796051502227783, 1.2238253355026245, 1.2091535329818726, 1.2020947933197021, 1.2382514476776123, 1.2239093780517578, 1.2028509378433228, 1.187124252319336, 1.2289257049560547, 1.1903051137924194, 1.2044090032577515, 1.2411627769470215, 1.1892215013504028, 1.2658562660217285, 1.1975646018981934, 1.2079997062683105, 1.1994794607162476, 1.1797175407409668, 1.2203519344329834, 1.2252451181411743, 1.165885329246521, 1.2501323223114014, 1.244278073310852, 1.1873661279678345, 1.1580657958984375, 1.2247276306152344, 1.1432141065597534, 1.2095718383789062, 1.158509373664856, 1.1655535697937012, 1.1659176349639893, 1.1490447521209717, 1.1967219114303589, 1.1390101909637451, 1.2221544981002808, 1.1698622703552246, 1.1883546113967896, 1.1086103916168213, 1.1209269762039185, 1.1354777812957764, 1.1392765045166016, 1.1299264430999756, 1.154364824295044, 1.1481118202209473, 1.1652874946594238, 1.1858667135238647, 1.15487802028656, 1.1847248077392578, 1.1525014638900757, 1.1462546586990356, 1.208011507987976, 1.120740532875061, 1.1659481525421143, 1.1694896221160889, 1.0828713178634644, 1.1748626232147217, 1.1503666639328003, 1.1466387510299683, 1.1454447507858276, 1.1696397066116333, 1.1216487884521484, 1.1778172254562378, 1.12550687789917, 1.1234385967254639, 1.1306135654449463, 1.151374340057373, 1.129196047782898, 1.1178187131881714, 1.170576572418213, 1.1899133920669556, 1.1707265377044678, 1.1164737939834595, 1.1959823369979858, 1.1681346893310547, 1.1587492227554321, 1.13798189163208, 1.1310572624206543, 1.101757526397705, 1.1414200067520142, 1.1491819620132446, 1.1347318887710571, 1.144582986831665, 1.2135539054870605, 1.1528359651565552, 1.1491725444793701, 1.0971646308898926, 1.1299971342086792, 1.1272705793380737, 1.1823867559432983, 1.107822299003601, 1.143808126449585, 1.183708667755127, 1.182350993156433, 1.1464463472366333, 1.1554292440414429, 1.1578797101974487, 1.142924189567566, 1.1583760976791382, 1.116803526878357, 1.1217275857925415, 1.1599228382110596, 1.1602386236190796, 1.0928070545196533, 1.2064967155456543, 1.1276291608810425, 1.0988706350326538, 1.134400725364685, 1.108317255973816, 1.0871161222457886, 1.0933092832565308, 1.1262867450714111, 1.1333701610565186, 1.1039879322052002, 1.073742389678955, 1.1127699613571167, 1.1287953853607178, 1.1257998943328857, 1.1457877159118652, 1.0797920227050781, 1.152708888053894, 1.0901762247085571, 1.1254538297653198, 1.1279784440994263, 1.096714735031128, 1.1294496059417725, 1.149990200996399, 1.085931658744812, 1.1651684045791626, 1.1656564474105835, 1.099032998085022, 1.0967556238174438, 1.1532440185546875, 1.058794617652893, 1.1413557529449463, 1.0955511331558228, 1.0858861207962036, 1.0878689289093018, 1.0953019857406616, 1.112755298614502, 1.0730915069580078, 1.1533280611038208, 1.0966721773147583, 1.1258833408355713, 1.0475966930389404, 1.076920747756958, 1.0718048810958862, 1.0825611352920532, 1.0711054801940918, 1.1107962131500244, 1.1145089864730835, 1.11556875705719, 1.1427000761032104, 1.1194103956222534, 1.128803014755249, 1.1054472923278809, 1.0895687341690063, 1.1486767530441284, 1.066338062286377, 1.090377926826477, 1.1212176084518433, 1.0430529117584229, 1.134710669517517, 1.1049774885177612, 1.1108638048171997, 1.0857901573181152, 1.131887435913086, 1.0732016563415527, 1.1171092987060547, 1.0897361040115356, 1.092767596244812, 1.0834623575210571, 1.1164450645446777, 1.0804239511489868, 1.0586599111557007, 1.1228883266448975, 1.1364542245864868, 1.1316262483596802, 1.0791130065917969, 1.148840308189392, 1.1143279075622559, 1.1345672607421875, 1.0880457162857056, 1.1035290956497192, 1.070821762084961, 1.1139042377471924, 1.1046146154403687, 1.0876104831695557, 1.1114853620529175, 1.184815526008606, 1.11515212059021, 1.1097328662872314, 1.0724132061004639, 1.1138157844543457, 1.1030707359313965, 1.1543787717819214, 1.0846253633499146, 1.0999208688735962, 1.1464715003967285, 1.159574270248413, 1.1209828853607178, 1.1269869804382324, 1.1181504726409912, 1.1185179948806763, 1.1158875226974487, 1.0918093919754028, 1.0812970399856567, 1.1428083181381226, 1.137992024421692, 1.0592832565307617, 1.174317479133606, 1.0980883836746216, 1.0894695520401, 1.1026321649551392, 1.0869839191436768, 1.06089448928833, 1.056842565536499, 1.095310091972351, 1.105133295059204, 1.0680564641952515, 1.057900309562683, 1.0811001062393188, 1.1034564971923828, 1.0923303365707397, 1.1148643493652344, 1.0669856071472168, 1.116156816482544, 1.0739737749099731, 1.1009570360183716, 1.1065723896026611, 1.0906051397323608, 1.1140174865722656, 1.1255912780761719, 1.0606743097305298, 1.1497641801834106, 1.1570602655410767, 1.0777688026428223, 1.0616450309753418, 1.1352461576461792, 1.042781114578247, 1.1220545768737793, 1.0833251476287842, 1.0663357973098755, 1.0788764953613281, 1.0692896842956543, 1.091247797012329, 1.0569725036621094, 1.1294175386428833, 1.07853364944458, 1.112281322479248, 1.034614086151123, 1.0561878681182861, 1.0543828010559082, 1.06678307056427, 1.0527244806289673, 1.0889866352081299, 1.1011576652526855, 1.091684103012085, 1.1085782051086426, 1.1019668579101562, 1.1126773357391357, 1.0777161121368408, 1.0751830339431763, 1.113885760307312, 1.059655785560608, 1.0796908140182495, 1.111220121383667, 1.0100595951080322, 1.0992857217788696, 1.0850275754928589, 1.0861220359802246, 1.0600358247756958, 1.1030808687210083, 1.0535531044006348, 1.1031873226165771, 1.0699183940887451, 1.0621223449707031, 1.081485390663147, 1.1017876863479614, 1.0578058958053589, 1.0516852140426636, 1.096803903579712, 1.1287809610366821, 1.1095774173736572, 1.0709348917007446, 1.1191712617874146, 1.1012804508209229, 1.1043802499771118, 1.0720446109771729, 1.0937153100967407, 1.0472238063812256, 1.1098629236221313, 1.0819891691207886, 1.0644214153289795, 1.0926986932754517]
Test accuracy: <built-in method append of list object at 0x0000020ABD67B848> Learning rate: 0.005 Accuracy: [0.19895833333333332, 0.28489583333333335, 0.37916666666666665, 0.4421875, 0.4609375, 0.4864583333333333, 0.47552083333333334, 0.5489583333333333, 0.5427083333333333, 0.5390625, 0.5286458333333334, 0.5692708333333333, 0.5515625, 0.5666666666666667, 0.5677083333333334, 0.5708333333333333, 0.5734375, 0.5833333333333334, 0.5708333333333333, 0.571875, 0.5984375, 0.5609375, 0.5802083333333333, 0.5963541666666666, 0.5994791666666667, 0.59375, 0.603125, 0.6114583333333333, 0.5880208333333333, 0.5947916666666667, 0.6109375, 0.6151041666666667, 0.603125, 0.6088541666666667, 0.6020833333333333, 0.6, 0.6078125, 0.60625, 0.596875, 0.6088541666666667, 0.6, 0.6296875, 0.615625, 0.5984375, 0.61875, 0.6, 0.5958333333333333, 0.61875, 0.6338541666666667, 0.6083333333333333, 0.6255208333333333, 0.6130208333333333, 0.6239583333333333, 0.6182291666666667, 0.6265625, 0.6270833333333333, 0.6161458333333333, 0.6197916666666666, 0.6046875, 0.6239583333333333, 0.615625, 0.6421875, 0.646875, 0.6427083333333333, 0.6213541666666667, 0.6333333333333333, 0.6213541666666667, 0.6270833333333333, 0.6234375, 0.6078125, 0.6260416666666667, 0.609375, 0.625, 0.6296875, 0.6166666666666667, 0.625, 0.61875, 0.6114583333333333, 0.6583333333333333, 0.6223958333333334, 0.625, 0.6208333333333333, 0.6208333333333333, 0.6255208333333333, 0.6307291666666667, 0.625, 0.6375, 0.6291666666666667, 0.6296875, 0.6223958333333334, 0.6359375, 0.6380208333333334, 0.61875, 0.6166666666666667, 0.6385416666666667, 0.6442708333333333, 0.6166666666666667, 0.625, 0.6260416666666667, 0.634375, 0.6307291666666667, 0.6463541666666667, 0.6354166666666666, 0.6255208333333333, 0.6270833333333333, 0.6182291666666667] Loss: [2.9492735862731934, 2.5147950649261475, 2.2038698196411133, 1.9358131885528564, 1.8041130304336548, 1.7125283479690552, 1.731528639793396, 1.5479799509048462, 1.5552260875701904, 1.5687271356582642, 1.5618761777877808, 1.452545404434204, 1.4814947843551636, 1.4482777118682861, 1.4253120422363281, 1.4047309160232544, 1.4020862579345703, 1.3614141941070557, 1.4362943172454834, 1.3988940715789795, 1.3186408281326294, 1.461355447769165, 1.3523186445236206, 1.3210557699203491, 1.3454664945602417, 1.2975122928619385, 1.2898181676864624, 1.2767020463943481, 1.2976982593536377, 1.2944061756134033, 1.255431890487671, 1.2333325147628784, 1.2922428846359253, 1.2640671730041504, 1.2716047763824463, 1.2747100591659546, 1.251641869544983, 1.3227430582046509, 1.261326551437378, 1.2777973413467407, 1.244700312614441, 1.225093126296997, 1.2872737646102905, 1.275636911392212, 1.2261983156204224, 1.2873055934906006, 1.2936145067214966, 1.2393615245819092, 1.2029153108596802, 1.254062294960022, 1.184658408164978, 1.2749577760696411, 1.2187117338180542, 1.1969231367111206, 1.2019906044006348, 1.1756123304367065, 1.2397938966751099, 1.210776686668396, 1.2520670890808105, 1.217203974723816, 1.2235558032989502, 1.1498043537139893, 1.1766492128372192, 1.1497634649276733, 1.1869702339172363, 1.1738277673721313, 1.210745930671692, 1.1871970891952515, 1.1938084363937378, 1.236845850944519, 1.1903326511383057, 1.2521027326583862, 1.168318510055542, 1.174338698387146, 1.2454760074615479, 1.186509609222412, 1.1972473859786987, 1.2165825366973877, 1.1289578676223755, 1.2005046606063843, 1.218538522720337, 1.1824277639389038, 1.1973063945770264, 1.2158229351043701, 1.1473510265350342, 1.188433051109314, 1.16960608959198, 1.1735678911209106, 1.1763724088668823, 1.2111449241638184, 1.1721206903457642, 1.1398448944091797, 1.1879801750183105, 1.2186295986175537, 1.2100566625595093, 1.1606426239013672, 1.2080408334732056, 1.1850594282150269, 1.1908767223358154, 1.1637698411941528, 1.1903679370880127, 1.1280640363693237, 1.1743743419647217, 1.1905597448349, 1.1702237129211426, 1.1907166242599487]
Test accuracy: <built-in method append of list object at 0x0000020ABD67B848> Learning rate: 0.005 Accuracy: [0.19895833333333332, 0.28489583333333335, 0.37916666666666665, 0.4421875, 0.4609375, 0.4864583333333333, 0.47552083333333334, 0.5489583333333333, 0.5427083333333333, 0.5390625, 0.5286458333333334, 0.5692708333333333, 0.5515625, 0.5666666666666667, 0.5677083333333334, 0.5708333333333333, 0.5734375, 0.5833333333333334, 0.5708333333333333, 0.571875, 0.5984375, 0.5609375, 0.5802083333333333, 0.5963541666666666, 0.5994791666666667, 0.59375, 0.603125, 0.6114583333333333, 0.5880208333333333, 0.5947916666666667, 0.6109375, 0.6151041666666667, 0.603125, 0.6088541666666667, 0.6020833333333333, 0.6, 0.6078125, 0.60625, 0.596875, 0.6088541666666667, 0.6, 0.6296875, 0.615625, 0.5984375, 0.61875, 0.6, 0.5958333333333333, 0.61875, 0.6338541666666667, 0.6083333333333333, 0.6255208333333333, 0.6130208333333333, 0.6239583333333333, 0.6182291666666667, 0.6265625, 0.6270833333333333, 0.6161458333333333, 0.6197916666666666, 0.6046875, 0.6239583333333333, 0.615625, 0.6421875, 0.646875, 0.6427083333333333, 0.6213541666666667, 0.6333333333333333, 0.6213541666666667, 0.6270833333333333, 0.6234375, 0.6078125, 0.6260416666666667, 0.609375, 0.625, 0.6296875, 0.6166666666666667, 0.625, 0.61875, 0.6114583333333333, 0.6583333333333333, 0.6223958333333334, 0.625, 0.6208333333333333, 0.6208333333333333, 0.6255208333333333, 0.6307291666666667, 0.625, 0.6375, 0.6291666666666667, 0.6296875, 0.6223958333333334, 0.6359375, 0.6380208333333334, 0.61875, 0.6166666666666667, 0.6385416666666667, 0.6442708333333333, 0.6166666666666667, 0.625, 0.6260416666666667, 0.634375, 0.6307291666666667, 0.6463541666666667, 0.6354166666666666, 0.6255208333333333, 0.6270833333333333, 0.6182291666666667, 0.6135416666666667, 0.6390625, 0.64375, 0.640625, 0.6421875, 0.6333333333333333, 0.6203125, 0.6375, 0.6359375, 0.6208333333333333, 0.6109375, 0.6375, 0.6270833333333333, 0.6333333333333333, 0.634375, 0.6192708333333333, 0.6473958333333333, 0.6447916666666667, 0.6338541666666667, 0.6239583333333333, 0.6453125, 0.6234375, 0.6322916666666667, 0.6390625, 0.646875, 0.6390625, 0.646875, 0.6421875, 0.6416666666666667, 0.6348958333333333, 0.653125, 0.6557291666666667, 0.6375, 0.6401041666666667, 0.6375, 0.6255208333333333, 0.64375, 0.6328125, 0.634375, 0.6385416666666667, 0.6401041666666667, 0.6583333333333333, 0.640625, 0.634375, 0.6479166666666667, 0.6328125, 0.6244791666666667, 0.6453125, 0.6541666666666667, 0.6385416666666667, 0.6552083333333333, 0.6348958333333333, 0.646875, 0.640625, 0.6447916666666667, 0.6578125, 0.6375, 0.65, 0.6291666666666667, 0.6411458333333333, 0.6375, 0.6536458333333334, 0.6578125, 0.6692708333333334, 0.6526041666666667, 0.6536458333333334, 0.6390625, 0.6421875, 0.6442708333333333, 0.6270833333333333, 0.6421875, 0.6307291666666667, 0.6416666666666667, 0.6494791666666667, 0.640625, 0.6453125, 0.6375, 0.6234375, 0.6625, 0.6411458333333333, 0.6427083333333333, 0.6442708333333333, 0.6515625, 0.63125, 0.6453125, 0.621875, 0.6640625, 0.6458333333333334, 0.6489583333333333, 0.6463541666666667, 0.6567708333333333, 0.6583333333333333, 0.6348958333333333, 0.6244791666666667, 0.6484375, 0.6484375, 0.6302083333333334, 0.6427083333333333, 0.6354166666666666, 0.6427083333333333, 0.6473958333333333, 0.6703125, 0.6375, 0.6348958333333333, 0.6473958333333333, 0.6255208333333333] Loss: [2.9492735862731934, 2.5147950649261475, 2.2038698196411133, 1.9358131885528564, 1.8041130304336548, 1.7125283479690552, 1.731528639793396, 1.5479799509048462, 1.5552260875701904, 1.5687271356582642, 1.5618761777877808, 1.452545404434204, 1.4814947843551636, 1.4482777118682861, 1.4253120422363281, 1.4047309160232544, 1.4020862579345703, 1.3614141941070557, 1.4362943172454834, 1.3988940715789795, 1.3186408281326294, 1.461355447769165, 1.3523186445236206, 1.3210557699203491, 1.3454664945602417, 1.2975122928619385, 1.2898181676864624, 1.2767020463943481, 1.2976982593536377, 1.2944061756134033, 1.255431890487671, 1.2333325147628784, 1.2922428846359253, 1.2640671730041504, 1.2716047763824463, 1.2747100591659546, 1.251641869544983, 1.3227430582046509, 1.261326551437378, 1.2777973413467407, 1.244700312614441, 1.225093126296997, 1.2872737646102905, 1.275636911392212, 1.2261983156204224, 1.2873055934906006, 1.2936145067214966, 1.2393615245819092, 1.2029153108596802, 1.254062294960022, 1.184658408164978, 1.2749577760696411, 1.2187117338180542, 1.1969231367111206, 1.2019906044006348, 1.1756123304367065, 1.2397938966751099, 1.210776686668396, 1.2520670890808105, 1.217203974723816, 1.2235558032989502, 1.1498043537139893, 1.1766492128372192, 1.1497634649276733, 1.1869702339172363, 1.1738277673721313, 1.210745930671692, 1.1871970891952515, 1.1938084363937378, 1.236845850944519, 1.1903326511383057, 1.2521027326583862, 1.168318510055542, 1.174338698387146, 1.2454760074615479, 1.186509609222412, 1.1972473859786987, 1.2165825366973877, 1.1289578676223755, 1.2005046606063843, 1.218538522720337, 1.1824277639389038, 1.1973063945770264, 1.2158229351043701, 1.1473510265350342, 1.188433051109314, 1.16960608959198, 1.1735678911209106, 1.1763724088668823, 1.2111449241638184, 1.1721206903457642, 1.1398448944091797, 1.1879801750183105, 1.2186295986175537, 1.2100566625595093, 1.1606426239013672, 1.2080408334732056, 1.1850594282150269, 1.1908767223358154, 1.1637698411941528, 1.1903679370880127, 1.1280640363693237, 1.1743743419647217, 1.1905597448349, 1.1702237129211426, 1.1907166242599487, 1.223036289215088, 1.169804334640503, 1.1548728942871094, 1.132008671760559, 1.1416395902633667, 1.1504175662994385, 1.2002902030944824, 1.14418625831604, 1.1602394580841064, 1.228546380996704, 1.2302987575531006, 1.1917146444320679, 1.1823699474334717, 1.1938217878341675, 1.1531784534454346, 1.2046080827713013, 1.1509277820587158, 1.150307536125183, 1.1905076503753662, 1.1971142292022705, 1.1303925514221191, 1.2379082441329956, 1.1618000268936157, 1.1211607456207275, 1.1663708686828613, 1.128930687904358, 1.1290210485458374, 1.1202577352523804, 1.1330646276474, 1.1645300388336182, 1.123727560043335, 1.0948551893234253, 1.1576712131500244, 1.1495565176010132, 1.1472455263137817, 1.1600799560546875, 1.1299482583999634, 1.1895382404327393, 1.1475437879562378, 1.1652659177780151, 1.1395343542099, 1.1074413061141968, 1.1467795372009277, 1.1625449657440186, 1.1327260732650757, 1.1646236181259155, 1.1958496570587158, 1.1381189823150635, 1.130185604095459, 1.152679204940796, 1.1061376333236694, 1.176528811454773, 1.1260945796966553, 1.1165707111358643, 1.1217328310012817, 1.0904957056045532, 1.146969199180603, 1.121395468711853, 1.165018916130066, 1.1366255283355713, 1.1560004949569702, 1.080734372138977, 1.1020363569259644, 1.079343318939209, 1.0975689888000488, 1.1070910692214966, 1.1492708921432495, 1.1341933012008667, 1.1406805515289307, 1.1634066104888916, 1.133597731590271, 1.1792800426483154, 1.106143832206726, 1.1179826259613037, 1.167278528213501, 1.1269652843475342, 1.1282824277877808, 1.1497162580490112, 1.058878779411316, 1.133429765701294, 1.1484071016311646, 1.1271026134490967, 1.1229366064071655, 1.1582063436508179, 1.1019574403762817, 1.1551845073699951, 1.1066792011260986, 1.1235473155975342, 1.119128942489624, 1.143121600151062, 1.113867998123169, 1.0779993534088135, 1.1348673105239868, 1.1679967641830444, 1.1474010944366455, 1.1085411310195923, 1.1587567329406738, 1.143706202507019, 1.146684169769287, 1.1157221794128418, 1.1395723819732666, 1.0740464925765991, 1.1392251253128052, 1.1401299238204956, 1.1112614870071411, 1.154883623123169]
Test accuracy: <built-in method append of list object at 0x0000020ABD67B848> Learning rate: 0.005 Accuracy: [0.19895833333333332, 0.28489583333333335, 0.37916666666666665, 0.4421875, 0.4609375, 0.4864583333333333, 0.47552083333333334, 0.5489583333333333, 0.5427083333333333, 0.5390625, 0.5286458333333334, 0.5692708333333333, 0.5515625, 0.5666666666666667, 0.5677083333333334, 0.5708333333333333, 0.5734375, 0.5833333333333334, 0.5708333333333333, 0.571875, 0.5984375, 0.5609375, 0.5802083333333333, 0.5963541666666666, 0.5994791666666667, 0.59375, 0.603125, 0.6114583333333333, 0.5880208333333333, 0.5947916666666667, 0.6109375, 0.6151041666666667, 0.603125, 0.6088541666666667, 0.6020833333333333, 0.6, 0.6078125, 0.60625, 0.596875, 0.6088541666666667, 0.6, 0.6296875, 0.615625, 0.5984375, 0.61875, 0.6, 0.5958333333333333, 0.61875, 0.6338541666666667, 0.6083333333333333, 0.6255208333333333, 0.6130208333333333, 0.6239583333333333, 0.6182291666666667, 0.6265625, 0.6270833333333333, 0.6161458333333333, 0.6197916666666666, 0.6046875, 0.6239583333333333, 0.615625, 0.6421875, 0.646875, 0.6427083333333333, 0.6213541666666667, 0.6333333333333333, 0.6213541666666667, 0.6270833333333333, 0.6234375, 0.6078125, 0.6260416666666667, 0.609375, 0.625, 0.6296875, 0.6166666666666667, 0.625, 0.61875, 0.6114583333333333, 0.6583333333333333, 0.6223958333333334, 0.625, 0.6208333333333333, 0.6208333333333333, 0.6255208333333333, 0.6307291666666667, 0.625, 0.6375, 0.6291666666666667, 0.6296875, 0.6223958333333334, 0.6359375, 0.6380208333333334, 0.61875, 0.6166666666666667, 0.6385416666666667, 0.6442708333333333, 0.6166666666666667, 0.625, 0.6260416666666667, 0.634375, 0.6307291666666667, 0.6463541666666667, 0.6354166666666666, 0.6255208333333333, 0.6270833333333333, 0.6182291666666667, 0.6135416666666667, 0.6390625, 0.64375, 0.640625, 0.6421875, 0.6333333333333333, 0.6203125, 0.6375, 0.6359375, 0.6208333333333333, 0.6109375, 0.6375, 0.6270833333333333, 0.6333333333333333, 0.634375, 0.6192708333333333, 0.6473958333333333, 0.6447916666666667, 0.6338541666666667, 0.6239583333333333, 0.6453125, 0.6234375, 0.6322916666666667, 0.6390625, 0.646875, 0.6390625, 0.646875, 0.6421875, 0.6416666666666667, 0.6348958333333333, 0.653125, 0.6557291666666667, 0.6375, 0.6401041666666667, 0.6375, 0.6255208333333333, 0.64375, 0.6328125, 0.634375, 0.6385416666666667, 0.6401041666666667, 0.6583333333333333, 0.640625, 0.634375, 0.6479166666666667, 0.6328125, 0.6244791666666667, 0.6453125, 0.6541666666666667, 0.6385416666666667, 0.6552083333333333, 0.6348958333333333, 0.646875, 0.640625, 0.6447916666666667, 0.6578125, 0.6375, 0.65, 0.6291666666666667, 0.6411458333333333, 0.6375, 0.6536458333333334, 0.6578125, 0.6692708333333334, 0.6526041666666667, 0.6536458333333334, 0.6390625, 0.6421875, 0.6442708333333333, 0.6270833333333333, 0.6421875, 0.6307291666666667, 0.6416666666666667, 0.6494791666666667, 0.640625, 0.6453125, 0.6375, 0.6234375, 0.6625, 0.6411458333333333, 0.6427083333333333, 0.6442708333333333, 0.6515625, 0.63125, 0.6453125, 0.621875, 0.6640625, 0.6458333333333334, 0.6489583333333333, 0.6463541666666667, 0.6567708333333333, 0.6583333333333333, 0.6348958333333333, 0.6244791666666667, 0.6484375, 0.6484375, 0.6302083333333334, 0.6427083333333333, 0.6354166666666666, 0.6427083333333333, 0.6473958333333333, 0.6703125, 0.6375, 0.6348958333333333, 0.6473958333333333, 0.6255208333333333, 0.6302083333333334, 0.6520833333333333, 0.6536458333333334, 0.6625, 0.6552083333333333, 0.6510416666666666, 0.6359375, 0.646875, 0.6385416666666667, 0.6375, 0.6270833333333333, 0.6494791666666667, 0.6375, 0.6484375, 0.6338541666666667, 0.6359375, 0.64375, 0.6510416666666666, 0.6453125, 0.6364583333333333, 0.6609375, 0.640625, 0.6520833333333333, 0.6473958333333333, 0.6489583333333333, 0.6411458333333333, 0.659375, 0.6526041666666667, 0.6484375, 0.6484375, 0.6520833333333333, 0.6604166666666667, 0.6494791666666667, 0.64375, 0.6442708333333333, 0.6442708333333333, 0.6421875, 0.6338541666666667, 0.6427083333333333, 0.6432291666666666, 0.6416666666666667, 0.6677083333333333, 0.6536458333333334, 0.640625, 0.6609375, 0.6453125, 0.6338541666666667, 0.6541666666666667, 0.6604166666666667, 0.6432291666666666, 0.6614583333333334, 0.6380208333333334, 0.6583333333333333, 0.6598958333333333, 0.6484375, 0.6614583333333334, 0.6479166666666667, 0.6640625, 0.6364583333333333, 0.6510416666666666, 0.6536458333333334, 0.6765625, 0.6625, 0.6666666666666666, 0.6489583333333333, 0.6630208333333333, 0.65, 0.6494791666666667, 0.6557291666666667, 0.6302083333333334, 0.6442708333333333, 0.6322916666666667, 0.65625, 0.6447916666666667, 0.64375, 0.6484375, 0.6520833333333333, 0.6375, 0.6786458333333333, 0.640625, 0.6479166666666667, 0.6526041666666667, 0.6447916666666667, 0.6453125, 0.653125, 0.6276041666666666, 0.6682291666666667, 0.6588541666666666, 0.6416666666666667, 0.6463541666666667, 0.6630208333333333, 0.6614583333333334, 0.6479166666666667, 0.6338541666666667, 0.6515625, 0.665625, 0.640625, 0.6526041666666667, 0.646875, 0.65, 0.653125, 0.6703125, 0.65, 0.6354166666666666, 0.6546875, 0.65] Loss: [2.9492735862731934, 2.5147950649261475, 2.2038698196411133, 1.9358131885528564, 1.8041130304336548, 1.7125283479690552, 1.731528639793396, 1.5479799509048462, 1.5552260875701904, 1.5687271356582642, 1.5618761777877808, 1.452545404434204, 1.4814947843551636, 1.4482777118682861, 1.4253120422363281, 1.4047309160232544, 1.4020862579345703, 1.3614141941070557, 1.4362943172454834, 1.3988940715789795, 1.3186408281326294, 1.461355447769165, 1.3523186445236206, 1.3210557699203491, 1.3454664945602417, 1.2975122928619385, 1.2898181676864624, 1.2767020463943481, 1.2976982593536377, 1.2944061756134033, 1.255431890487671, 1.2333325147628784, 1.2922428846359253, 1.2640671730041504, 1.2716047763824463, 1.2747100591659546, 1.251641869544983, 1.3227430582046509, 1.261326551437378, 1.2777973413467407, 1.244700312614441, 1.225093126296997, 1.2872737646102905, 1.275636911392212, 1.2261983156204224, 1.2873055934906006, 1.2936145067214966, 1.2393615245819092, 1.2029153108596802, 1.254062294960022, 1.184658408164978, 1.2749577760696411, 1.2187117338180542, 1.1969231367111206, 1.2019906044006348, 1.1756123304367065, 1.2397938966751099, 1.210776686668396, 1.2520670890808105, 1.217203974723816, 1.2235558032989502, 1.1498043537139893, 1.1766492128372192, 1.1497634649276733, 1.1869702339172363, 1.1738277673721313, 1.210745930671692, 1.1871970891952515, 1.1938084363937378, 1.236845850944519, 1.1903326511383057, 1.2521027326583862, 1.168318510055542, 1.174338698387146, 1.2454760074615479, 1.186509609222412, 1.1972473859786987, 1.2165825366973877, 1.1289578676223755, 1.2005046606063843, 1.218538522720337, 1.1824277639389038, 1.1973063945770264, 1.2158229351043701, 1.1473510265350342, 1.188433051109314, 1.16960608959198, 1.1735678911209106, 1.1763724088668823, 1.2111449241638184, 1.1721206903457642, 1.1398448944091797, 1.1879801750183105, 1.2186295986175537, 1.2100566625595093, 1.1606426239013672, 1.2080408334732056, 1.1850594282150269, 1.1908767223358154, 1.1637698411941528, 1.1903679370880127, 1.1280640363693237, 1.1743743419647217, 1.1905597448349, 1.1702237129211426, 1.1907166242599487, 1.223036289215088, 1.169804334640503, 1.1548728942871094, 1.132008671760559, 1.1416395902633667, 1.1504175662994385, 1.2002902030944824, 1.14418625831604, 1.1602394580841064, 1.228546380996704, 1.2302987575531006, 1.1917146444320679, 1.1823699474334717, 1.1938217878341675, 1.1531784534454346, 1.2046080827713013, 1.1509277820587158, 1.150307536125183, 1.1905076503753662, 1.1971142292022705, 1.1303925514221191, 1.2379082441329956, 1.1618000268936157, 1.1211607456207275, 1.1663708686828613, 1.128930687904358, 1.1290210485458374, 1.1202577352523804, 1.1330646276474, 1.1645300388336182, 1.123727560043335, 1.0948551893234253, 1.1576712131500244, 1.1495565176010132, 1.1472455263137817, 1.1600799560546875, 1.1299482583999634, 1.1895382404327393, 1.1475437879562378, 1.1652659177780151, 1.1395343542099, 1.1074413061141968, 1.1467795372009277, 1.1625449657440186, 1.1327260732650757, 1.1646236181259155, 1.1958496570587158, 1.1381189823150635, 1.130185604095459, 1.152679204940796, 1.1061376333236694, 1.176528811454773, 1.1260945796966553, 1.1165707111358643, 1.1217328310012817, 1.0904957056045532, 1.146969199180603, 1.121395468711853, 1.165018916130066, 1.1366255283355713, 1.1560004949569702, 1.080734372138977, 1.1020363569259644, 1.079343318939209, 1.0975689888000488, 1.1070910692214966, 1.1492708921432495, 1.1341933012008667, 1.1406805515289307, 1.1634066104888916, 1.133597731590271, 1.1792800426483154, 1.106143832206726, 1.1179826259613037, 1.167278528213501, 1.1269652843475342, 1.1282824277877808, 1.1497162580490112, 1.058878779411316, 1.133429765701294, 1.1484071016311646, 1.1271026134490967, 1.1229366064071655, 1.1582063436508179, 1.1019574403762817, 1.1551845073699951, 1.1066792011260986, 1.1235473155975342, 1.119128942489624, 1.143121600151062, 1.113867998123169, 1.0779993534088135, 1.1348673105239868, 1.1679967641830444, 1.1474010944366455, 1.1085411310195923, 1.1587567329406738, 1.143706202507019, 1.146684169769287, 1.1157221794128418, 1.1395723819732666, 1.0740464925765991, 1.1392251253128052, 1.1401299238204956, 1.1112614870071411, 1.154883623123169, 1.1812385320663452, 1.125274896621704, 1.1195948123931885, 1.0820655822753906, 1.1074488162994385, 1.105064034461975, 1.16074800491333, 1.0966004133224487, 1.1144633293151855, 1.1877410411834717, 1.1809247732162476, 1.136098027229309, 1.1466385126113892, 1.1582077741622925, 1.1224329471588135, 1.1555641889572144, 1.1203340291976929, 1.1268259286880493, 1.1466593742370605, 1.1727099418640137, 1.0832667350769043, 1.2122234106063843, 1.1343270540237427, 1.0955857038497925, 1.1227264404296875, 1.1112608909606934, 1.104468584060669, 1.0969587564468384, 1.104713797569275, 1.1285454034805298, 1.1069836616516113, 1.083413004875183, 1.1213245391845703, 1.125434160232544, 1.1133785247802734, 1.1377017498016357, 1.0977168083190918, 1.1572757959365845, 1.1174055337905884, 1.1431318521499634, 1.1158757209777832, 1.0825186967849731, 1.1215693950653076, 1.1220066547393799, 1.0920602083206177, 1.1377739906311035, 1.1640197038650513, 1.1124396324157715, 1.0888985395431519, 1.1298378705978394, 1.0833814144134521, 1.1552094221115112, 1.0973632335662842, 1.1031540632247925, 1.098539113998413, 1.0548245906829834, 1.1294887065887451, 1.0962505340576172, 1.1435743570327759, 1.1205058097839355, 1.113369345664978, 1.037178635597229, 1.0839186906814575, 1.0508644580841064, 1.071422815322876, 1.0771403312683105, 1.1251729726791382, 1.1216291189193726, 1.112144112586975, 1.1463780403137207, 1.115643858909607, 1.1512292623519897, 1.0872364044189453, 1.093329668045044, 1.142299771308899, 1.1029106378555298, 1.0952656269073486, 1.1328092813491821, 1.0272209644317627, 1.1172802448272705, 1.125410556793213, 1.1007349491119385, 1.1242105960845947, 1.1394212245941162, 1.080430269241333, 1.146764874458313, 1.088370442390442, 1.0921913385391235, 1.1050416231155396, 1.1167908906936646, 1.0875169038772583, 1.0570372343063354, 1.122162938117981, 1.15194833278656, 1.1305310726165771, 1.0832961797714233, 1.1424998044967651, 1.1062008142471313, 1.1175514459609985, 1.0894966125488281, 1.112636923789978, 1.058570384979248, 1.1132903099060059, 1.1333386898040771, 1.0880532264709473, 1.1064257621765137]
